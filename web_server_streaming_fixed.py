#!/usr/bin/env python3
"""
STT + LLM í‚¤ì›Œë“œ ì¶”ì¶œ í†µí•© ì„œë²„ (ìŠ¤íŠ¸ë¦¬ë° ë²„ì „) - Fixed WebSocket
- WebSocket audio format fix
- Proper base64 decoding
- Better error handling
"""

import json
import time
import torch
import numpy as np
import soundfile as sf
import os
import requests
import base64
from flask import Flask, render_template, request, jsonify
from flask_sock import Sock
from faster_whisper import WhisperModel
import warnings
import re
warnings.filterwarnings("ignore")

# í™˜ê²½ ë³€ìˆ˜
VLLM_API_URL = os.getenv("VLLM_API_URL", "http://localhost:8000/v1/completions")

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key'
sock = Sock(app)

# ì „ì—­ ë³€ìˆ˜
stt_processor = None
keyword_extractor = None

class VoiceActivityDetector:
    """ë¯¼ê°í•œ VAD êµ¬í˜„"""

    def __init__(self, sample_rate=16000):
        self.sample_rate = sample_rate
        self.energy_threshold = 0.001  # ë§¤ìš° ë‚®ì€ ì„ê³„ê°’
        self.silence_duration = 0.8  # ì¹¨ë¬µ ê°ì§€ ì‹œê°„

    def is_speech(self, audio_chunk):
        """ìŒì„±ì¸ì§€ íŒë‹¨"""
        if len(audio_chunk) == 0:
            return False
        energy = np.sqrt(np.mean(np.square(audio_chunk)))
        return energy > self.energy_threshold

class StreamingSTT:
    """ìŠ¤íŠ¸ë¦¬ë° STT ì²˜ë¦¬"""

    def __init__(self):
        self.model = None
        self.device = None
        self.vad = VoiceActivityDetector()
        self.audio_buffer = []
        self.silence_frames = 0
        self.silence_threshold = 12  # ì•½ 0.8ì´ˆ
        self.min_speech_length = 4800  # ìµœì†Œ 0.3ì´ˆ
        self.max_buffer_size = 160000  # ìµœëŒ€ 10ì´ˆ

        # í• ë£¨ì‹œë„¤ì´ì…˜ íŒ¨í„´
        self.hallucination_patterns = [
            "ê°ì‚¬í•©ë‹ˆë‹¤", "ì‹œì²­í•´ì£¼ì…”ì„œ", "êµ¬ë…", "ì¢‹ì•„ìš”",
            "MBC ë‰´ìŠ¤", "KBS ë‰´ìŠ¤", "ë‹¤ìŒ ì˜ìƒ", "ì˜ìƒí¸ì§‘"
        ]

    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        print("ğŸ”„ Whisper ëª¨ë¸ ë¡œë”© ì¤‘...")

        if torch.cuda.is_available():
            print(f"âœ… GPU ê°ì§€ë¨: {torch.cuda.get_device_name(0)}")
            device = "cuda"
            compute_type = "float16"
        else:
            device = "cpu"
            compute_type = "int8"

        self.model = WhisperModel(
            "medium",
            device=device,
            device_index=0 if device == "cuda" else None,
            compute_type=compute_type,
            download_root="/app/models",
            num_workers=1,
            cpu_threads=0
        )
        print(f"âœ… {device.upper()} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        self.device = device
        return True

    def process_stream(self, audio_chunk):
        """ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬"""
        if self.model is None:
            return None

        # VAD ì²´í¬
        is_speech = self.vad.is_speech(audio_chunk)

        if is_speech:
            # ìŒì„±ì´ ê°ì§€ë˜ë©´ ë²„í¼ì— ì¶”ê°€
            self.audio_buffer.extend(audio_chunk)
            self.silence_frames = 0

            # ë²„í¼ í¬ê¸° ì œí•œ
            if len(self.audio_buffer) > self.max_buffer_size:
                return self._process_buffer()

        else:
            # ì¹¨ë¬µ ì¹´ìš´íŠ¸
            self.silence_frames += 1

            # ì¶©ë¶„í•œ ì¹¨ë¬µì´ ê°ì§€ë˜ê³  ë²„í¼ì— ë°ì´í„°ê°€ ìˆìœ¼ë©´
            if self.silence_frames > self.silence_threshold and len(self.audio_buffer) > self.min_speech_length:
                return self._process_buffer()

        return None

    def _process_buffer(self):
        """ë²„í¼ ì²˜ë¦¬"""
        if not self.audio_buffer or len(self.audio_buffer) < self.min_speech_length:
            self.audio_buffer = []
            return None

        # ë²„í¼ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        audio_data = np.array(self.audio_buffer, dtype=np.float32)

        # ì •ê·œí™”
        max_val = np.max(np.abs(audio_data))
        if max_val > 0:
            audio_data = audio_data / max_val

        # ì„ì‹œ íŒŒì¼ ì €ì¥
        temp_file = f"/tmp/audio_{time.time()}.wav"
        sf.write(temp_file, audio_data, 16000)

        try:
            # Whisper ì²˜ë¦¬
            segments, info = self.model.transcribe(
                temp_file,
                language="ko",
                beam_size=5,
                best_of=5,
                patience=1,
                length_penalty=1,
                temperature=[0.0],
                compression_ratio_threshold=2.4,
                log_prob_threshold=-1.0,
                no_speech_threshold=0.6,
                condition_on_previous_text=False,
                word_timestamps=False
            )

            # ì „ì‚¬ ê²°ê³¼ ìˆ˜ì§‘
            full_text = []
            for segment in segments:
                text = segment.text.strip()
                if text and not self._is_hallucination(text):
                    full_text.append(text)

            # ë²„í¼ ì´ˆê¸°í™”
            self.audio_buffer = []

            if full_text:
                result_text = " ".join(full_text)
                return {
                    "text": result_text,
                    "language": info.language if info else "ko",
                    "timestamp": time.time()
                }

        except Exception as e:
            print(f"âŒ Whisper ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            self.audio_buffer = []

        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_file):
                os.remove(temp_file)

        return None

    def _is_hallucination(self, text):
        """í• ë£¨ì‹œë„¤ì´ì…˜ ì²´í¬"""
        for pattern in self.hallucination_patterns:
            if pattern in text:
                return True
        return False

class KeywordExtractor:
    """í‚¤ì›Œë“œ ì¶”ì¶œ (LLM ì‚¬ìš©)"""

    def __init__(self):
        self.min_importance = 0.3
        self.vllm_url = VLLM_API_URL

    def extract_keywords(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not text or len(text) < 5:
            return []

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.

í…ìŠ¤íŠ¸: "{text}"

ì‘ë‹µ í˜•ì‹:
{{
    "keywords": [
        {{"keyword": "ë‹¨ì–´1", "importance": 0.9}},
        {{"keyword": "ë‹¨ì–´2", "importance": 0.7}}
    ]
}}

í‚¤ì›Œë“œ:"""

        try:
            response = requests.post(
                self.vllm_url,
                json={
                    "model": "Qwen/Qwen2.5-7B-Instruct",
                    "prompt": prompt,
                    "max_tokens": 100,
                    "temperature": 0.1,
                    "stop": ["}", "}}"]
                },
                timeout=5
            )

            if response.status_code == 200:
                text_response = response.json()['choices'][0]['text']

                # JSON íŒŒì‹± ì‹œë„
                json_pattern = r'\{[^{}]*"keywords"[^{}]*\[[^\]]*\][^{}]*\}'
                json_match = re.search(json_pattern, text_response, re.DOTALL)

                if json_match:
                    data = json.loads(json_match.group(0))
                    keywords = data.get('keywords', [])
                    return [k for k in keywords if k.get('importance', 0) >= self.min_importance]

        except Exception as e:
            print(f"âŒ LLM í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")

        return []

# WebSocket í•¸ë“¤ëŸ¬
@sock.route('/ws')
def websocket(ws):
    """WebSocket ì—°ê²° ì²˜ë¦¬"""
    print(f"ğŸ”Œ ìƒˆ WebSocket ì—°ê²°: {request.remote_addr}")

    # ì „ì—­ ë³€ìˆ˜ë¡œ ë¯¸ë¦¬ ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©
    global stt_processor, keyword_extractor

    if stt_processor is None:
        streaming_stt = StreamingSTT()
        streaming_stt.load_model()
    else:
        streaming_stt = stt_processor

    if keyword_extractor is None:
        keyword_extractor = KeywordExtractor()

    audio_buffer = []
    process_chunk_size = 1600  # 100ms ë‹¨ìœ„ë¡œ ì²˜ë¦¬
    connection_active = True

    try:
        while connection_active:
            try:
                message = ws.receive(timeout=1)
                if message is None:
                    continue

                data = json.loads(message)

                if data.get('type') == 'audio':
                    # Base64 ì˜¤ë””ì˜¤ ë°ì´í„° ë””ì½”ë”©
                    try:
                        audio_bytes = base64.b64decode(data['data'])

                        # Float32 ë°°ì—´ë¡œ ë³€í™˜ (JavaScriptì—ì„œ Float32Arrayë¡œ ì „ì†¡ë¨)
                        audio_array = np.frombuffer(audio_bytes, dtype=np.float32)

                        # ë””ë²„ê·¸: ì˜¤ë””ì˜¤ ë°ì´í„° ìˆ˜ì‹  í™•ì¸
                        if len(audio_buffer) == 0:
                            print(f"ğŸµ ì²« ì˜¤ë””ì˜¤ ë°ì´í„° ìˆ˜ì‹ ! í¬ê¸°: {len(audio_array)}, í‰ê· : {np.mean(np.abs(audio_array)):.6f}")

                        # ë²„í¼ì— ì¶”ê°€
                        audio_buffer.extend(audio_array)

                        # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ëª¨ì´ë©´ ì²˜ë¦¬
                        while len(audio_buffer) >= process_chunk_size:
                            chunk = audio_buffer[:process_chunk_size]
                            audio_buffer = audio_buffer[process_chunk_size:]

                            # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
                            result = streaming_stt.process_stream(chunk)

                            if result:
                                print(f"âœ… ì „ì‚¬ ì™„ë£Œ: {result['text']}")

                                # í‚¤ì›Œë“œ ì¶”ì¶œ
                                keywords = keyword_extractor.extract_keywords(result['text'])
                                result['keywords'] = keywords

                                # ê²°ê³¼ ì „ì†¡
                                ws.send(json.dumps({
                                    "type": "transcription",
                                    **result
                                }))

                    except Exception as e:
                        print(f"âŒ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

                elif data.get('type') == 'config':
                    # ì„¤ì • ë©”ì‹œì§€ ì²˜ë¦¬
                    print(f"ğŸ“‹ ì„¤ì • ìˆ˜ì‹ : {data}")

            except json.JSONDecodeError as e:
                print(f"âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            except TimeoutError:
                # íƒ€ì„ì•„ì›ƒì€ ì •ìƒ - ê³„ì† ì§„í–‰
                pass
            except Exception as e:
                if "Connection closed" in str(e):
                    connection_active = False
                else:
                    print(f"âš ï¸ ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    except Exception as e:
        print(f"âŒ WebSocket ì˜¤ë¥˜: {e}")
    finally:
        print(f"ğŸ”Œ WebSocket ì—°ê²° ì¢…ë£Œ: {request.remote_addr}")

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    return render_template('index_keywords_scroll.html')

@app.route('/api/config', methods=['GET'])
def get_config():
    """ì„¤ì • ì •ë³´"""
    return jsonify({
        "sample_rate": 16000,
        "language": "ko",
        "model": "medium",
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        "gpu": torch.cuda.is_available(),
        "llm_enabled": True,
        "mode": "streaming",
        "buffer_mode": True
    })

@app.route('/api/test-audio', methods=['POST'])
def test_audio():
    """ì˜¤ë””ì˜¤ íŒŒì¼ í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    global stt_processor, keyword_extractor

    if stt_processor is None:
        stt_processor = StreamingSTT()
        stt_processor.load_model()

    if keyword_extractor is None:
        keyword_extractor = KeywordExtractor()

    # íŒŒì¼ ë°›ê¸°
    if 'audio' not in request.files:
        return jsonify({"error": "No audio file"}), 400

    audio_file = request.files['audio']

    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    temp_path = f"/tmp/test_{time.time()}.wav"
    audio_file.save(temp_path)

    try:
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        audio_data, sr = sf.read(temp_path)

        # 16kHzë¡œ ë¦¬ìƒ˜í”Œë§ (í•„ìš”í•œ ê²½ìš°)
        if sr != 16000:
            from scipy import signal
            audio_data = signal.resample(audio_data, int(len(audio_data) * 16000 / sr))

        # í”„ë¡œì„¸ì‹±
        stt_processor.audio_buffer = list(audio_data)
        result = stt_processor._process_buffer()

        if result:
            keywords = keyword_extractor.extract_keywords(result['text'])
            result['keywords'] = keywords
            return jsonify(result)
        else:
            return jsonify({"error": "No transcription"}), 500

    finally:
        if os.path.exists(temp_path):
            os.remove(temp_path)

if __name__ == '__main__':
    print("\n" + "="*60)
    print("ğŸ¤ STT ìŠ¤íŠ¸ë¦¬ë° ì„œë²„ (WebSocket Fixed)")
    print("="*60 + "\n")

    if torch.cuda.is_available():
        print(f"ğŸ–¥ï¸ GPU: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ“Š GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")

    # ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ
    print("\nğŸš€ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    stt_processor = StreamingSTT()
    stt_processor.load_model()
    keyword_extractor = KeywordExtractor()
    print("âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!\n")

    # ì„œë²„ ì‹œì‘
    print(f"ğŸŒ ì„œë²„ ì‹œì‘: http://localhost:5000")
    print(f"ğŸ”‡ VAD ê¸°ë°˜ ìŒì„± ê°ì§€")
    print(f"ğŸ”Œ WebSocket ì—”ë“œí¬ì¸íŠ¸: ws://localhost:5000/ws")
    print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸: POST /api/test-audio")
    print("="*60 + "\n")

    app.run(host='0.0.0.0', port=5000, debug=False)