#!/usr/bin/env python3
"""
ì´ˆì €ì§€ì—° ì‹¤ì‹œê°„ STT ì›¹ ì„œë²„ - ìµœì í™” ë²„ì „
"""

import os
import json
import numpy as np
import time
from pathlib import Path
from flask import Flask, render_template
from flask_cors import CORS
from flask_sock import Sock
from faster_whisper import WhisperModel
import base64
import io
import soundfile as sf
import torch
import threading
from queue import Queue
import asyncio

# Flask ì•± ìƒì„±
app = Flask(__name__, template_folder='templates', static_folder='static')
CORS(app, resources={r"/*": {"origins": "*"}})
sock = Sock(app)

class RealtimeWhisperProcessor:
    """ì´ˆì €ì§€ì—° ì‹¤ì‹œê°„ Whisper ì²˜ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self):
        self.model = None
        self.device = "cuda"
        self.sample_rate = 16000
        self.processing_queue = Queue()
        self.worker_thread = None

    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        print("ğŸ”„ ì´ˆì €ì§€ì—° Faster-Whisper ëª¨ë¸ ë¡œë”© ì¤‘ (GPU ëª¨ë“œ)...")

        # GPU ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
        if torch.cuda.is_available():
            print(f"âœ… GPU ê°ì§€ë¨: {torch.cuda.get_device_name(0)}")
            print(f"ğŸ“Š VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
            device = "cuda"
            compute_type = "float16"
        else:
            print("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            device = "cpu"
            compute_type = "int8"

        try:
            self.model = WhisperModel(
                "large-v3",
                device=device,
                device_index=0,
                compute_type=compute_type,
                download_root="/app/models",
                num_workers=1,
                cpu_threads=0
            )
            print(f"âœ… {device.upper()} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            self.device = device

            # ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘
            self.start_worker()
            return True
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def start_worker(self):
        """ì²˜ë¦¬ ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘"""
        self.worker_thread = threading.Thread(target=self._process_audio_queue, daemon=True)
        self.worker_thread.start()
        print("ğŸ”§ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì›Œì»¤ ì‹œì‘")

    def _process_audio_queue(self):
        """íì—ì„œ ì˜¤ë””ì˜¤ ì²˜ë¦¬ (ë°±ê·¸ë¼ìš´ë“œ)"""
        while True:
            try:
                audio_data, callback = self.processing_queue.get()
                if audio_data is not None:
                    result = self._transcribe_chunk(audio_data)
                    if callback:
                        callback(result)
            except Exception as e:
                print(f"âŒ ì›Œì»¤ ì˜¤ë¥˜: {e}")

    def _transcribe_chunk(self, audio_data):
        """ì‹¤ì œ ì „ì‚¬ ì²˜ë¦¬"""
        if self.model is None:
            return None

        # ì˜¤ë””ì˜¤ ì •ê·œí™”
        audio_data = audio_data / np.max(np.abs(audio_data) + 1e-10)

        # ì„ì‹œ íŒŒì¼ ì €ì¥
        temp_file = f"/tmp/audio_{time.time()}.wav"
        sf.write(temp_file, audio_data, self.sample_rate)

        try:
            # ì´ˆì €ì§€ì—° ì„¤ì •ìœ¼ë¡œ Transcribe
            segments, info = self.model.transcribe(
                temp_file,
                language="ko",
                task="transcribe",
                beam_size=2,  # ê· í˜•ì  (ì†ë„ì™€ ì •í™•ë„)
                best_of=2,    # ì ì ˆí•œ í›„ë³´ íƒìƒ‰
                temperature=0.0,
                vad_filter=True,
                vad_parameters={
                    "threshold": 0.4,  # ì ì ˆí•œ ë¯¼ê°ë„
                    "min_speech_duration_ms": 150,  # ìµœì†Œ ë°œí™” ê¸¸ì´
                    "min_silence_duration_ms": 200,  # ìì—°ìŠ¤ëŸ¬ìš´ ì¹¨ë¬µ êµ¬ê°„
                    "speech_pad_ms": 100  # ì ì ˆí•œ íŒ¨ë”©
                },
                condition_on_previous_text=False,
                initial_prompt=None,
                word_timestamps=False  # íƒ€ì„ìŠ¤íƒ¬í”„ ë¹„í™œì„±í™” (ì†ë„ í–¥ìƒ)
            )

            # ê²°ê³¼ ìˆ˜ì§‘
            text = ""
            for segment in segments:
                text += segment.text

            # í›„ì²˜ë¦¬
            text = text.strip().replace("  ", " ")

            result = {
                "text": text,
                "language": info.language if info.language else "ko",
                "confidence": float(info.language_probability) if info.language_probability else 0.95,
                "device": self.device,
                "processing_time": time.time()
            }

            return result

        except Exception as e:
            print(f"âŒ Transcribe ì˜¤ë¥˜: {e}")
            return None
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            Path(temp_file).unlink(missing_ok=True)

    def process_audio_async(self, audio_data, callback):
        """ë¹„ë™ê¸° ì˜¤ë””ì˜¤ ì²˜ë¦¬"""
        self.processing_queue.put((audio_data, callback))

# Whisper í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤
processor = RealtimeWhisperProcessor()

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    return render_template('index_fixed.html')

@app.route('/api/config')
def get_config():
    """ì„¤ì • ì •ë³´ ì œê³µ"""
    return {
        'status': 'ready',
        'websocket': 'integrated',
        'device': processor.device,
        'gpu': torch.cuda.is_available(),
        'mode': 'ultra_realtime'
    }

@sock.route('/ws')
def websocket(ws):
    """WebSocket ì—°ê²° ì²˜ë¦¬"""
    print(f"âœ… í´ë¼ì´ì–¸íŠ¸ ì—°ê²°")
    print(f"ğŸ“¡ ì´ˆì €ì§€ì—° ëª¨ë“œ í™œì„±í™”: {time.strftime('%Y-%m-%d %H:%M:%S')}")

    audio_buffer = []
    buffer_duration = 0
    max_buffer_duration = 1.2  # 1.2ì´ˆë§ˆë‹¤ ì²˜ë¦¬ (ê· í˜•ì )
    min_buffer_duration = 0.8  # ìµœì†Œ 0.8ì´ˆ ë²„í¼ (ë¬¸ë§¥ í™•ë³´)
    last_result_text = ""  # ì¤‘ë³µ ì œê±°ìš©

    def send_result(result):
        """ê²°ê³¼ ì „ì†¡ ì½œë°±"""
        nonlocal last_result_text
        if result and result["text"] and result["text"] != last_result_text:
            response = {
                "type": "transcription",
                "text": result["text"],
                "language": result["language"],
                "confidence": result["confidence"],
                "device": result["device"],
                "timestamp": time.time(),
                "latency": time.time() - result.get("processing_time", time.time())
            }
            try:
                ws.send(json.dumps(response, ensure_ascii=False))
                print(f"ğŸ“ [{result['device'].upper()}] {result['text'][:50]}... (ì§€ì—°: {response['latency']:.2f}ì´ˆ)")
                last_result_text = result["text"]
            except:
                pass

    while True:
        try:
            message = ws.receive()
            if message is None:
                break

            # JSON ë©”ì‹œì§€ ì²˜ë¦¬
            try:
                data = json.loads(message)

                if data.get("type") == "audio":
                    # Base64 ì˜¤ë””ì˜¤ ë°ì´í„° ë””ì½”ë“œ
                    audio_base64 = data.get("data", "")
                    audio_bytes = base64.b64decode(audio_base64)

                    # Float32 ë°°ì—´ë¡œ ë³€í™˜
                    audio_chunk = np.frombuffer(audio_bytes, dtype=np.float32)
                    audio_buffer.extend(audio_chunk)

                    # ë²„í¼ ì‹œê°„ ê³„ì‚°
                    buffer_duration = len(audio_buffer) / 16000

                    # ë™ì  ë²„í¼ ì²˜ë¦¬
                    # VADê°€ ìŒì„±ì„ ê°ì§€í•˜ë©´ ë” ì§§ì€ ê°„ê²©ìœ¼ë¡œ ì²˜ë¦¬
                    if buffer_duration >= min_buffer_duration:
                        # ìŒì„± ì—ë„ˆì§€ ì²´í¬ (ê°„ë‹¨í•œ VAD)
                        energy = np.sqrt(np.mean(np.square(audio_chunk)))

                        if energy > 0.01 or buffer_duration >= max_buffer_duration:
                            # ìŒì„±ì´ ê°ì§€ë˜ê±°ë‚˜ ìµœëŒ€ ë²„í¼ ë„ë‹¬ì‹œ ì²˜ë¦¬
                            audio_array = np.array(audio_buffer)

                            # ë¹„ë™ê¸° ì²˜ë¦¬ íì— ì¶”ê°€
                            processor.process_audio_async(audio_array, send_result)

                            # ë²„í¼ ì´ˆê¸°í™” (ì¼ë¶€ ì˜¤ë²„ë© ìœ ì§€)
                            overlap_samples = int(0.1 * 16000)  # 0.1ì´ˆ ì˜¤ë²„ë©
                            audio_buffer = audio_buffer[-overlap_samples:] if len(audio_buffer) > overlap_samples else []
                            buffer_duration = len(audio_buffer) / 16000

                elif data.get("type") == "config":
                    # ì„¤ì • ì—…ë°ì´íŠ¸
                    ws.send(json.dumps({
                        "type": "config_updated",
                        "language": "ko",
                        "device": processor.device,
                        "mode": "ultra_realtime"
                    }))

            except json.JSONDecodeError as e:
                print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")

        except Exception as e:
            print(f"âŒ WebSocket ì˜¤ë¥˜: {e}")
            break

    print(f"âŒ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸš€ ì´ˆì €ì§€ì—° ì‹¤ì‹œê°„ STT ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜")
    print("Ultra-Low Latency Real-time Speech-to-Text")
    print("=" * 60)

    # GPU ì •ë³´ ì¶œë ¥
    if torch.cuda.is_available():
        print(f"ğŸ–¥ï¸ GPU: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ“Š VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print("âš¡ ì´ˆì €ì§€ì—° ëª¨ë“œ í™œì„±í™”")
    else:
        print("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # í…œí”Œë¦¿ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("templates", exist_ok=True)
    os.makedirs("static", exist_ok=True)

    # ëª¨ë¸ ë¡œë“œ
    processor.load_model()

    # Flask + WebSocket ì„œë²„ ì‹œì‘
    print(f"\nğŸŒ ìµœì í™”ëœ ì‹¤ì‹œê°„ STT ì„œë²„ ì‹œì‘: http://localhost:5000")
    print(f"ğŸ–¥ï¸ Device: {processor.device.upper()}")
    print(f"âš¡ ì§€ì—°ì‹œê°„: ~1ì´ˆ (ì •í™•ë„ì™€ ì†ë„ì˜ ê· í˜•)")
    print("WebSocketì€ /ws ê²½ë¡œì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤.\n")

    app.run(host='0.0.0.0', port=5000, debug=False)

if __name__ == "__main__":
    main()