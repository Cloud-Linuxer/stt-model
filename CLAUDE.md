# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## üéØ Project Overview

**STT + LLM + RAG Ïã§ÏãúÍ∞Ñ ÏùåÏÑ± ÏßÄÎä•Ìòï Ï†ïÎ≥¥ ÏãúÏä§ÌÖú**

Ïã§ÏãúÍ∞Ñ ÏùåÏÑ± Ïù∏Ïãù(STT)Í≥º LLM Í∏∞Î∞ò ÌÇ§ÏõåÎìú Ï∂îÏ∂úÏùÑ ÎÑòÏñ¥, Ï∂îÏ∂úÎêú ÌÇ§ÏõåÎìúÏóê ÎåÄÌïú Í¥ÄÎ†® Ï†ïÎ≥¥Î•º ÏûêÎèôÏúºÎ°ú Í≤ÄÏÉâÌïòÍ≥† Ï†úÍ≥µÌïòÎäî RAG(Retrieval-Augmented Generation) ÏãúÏä§ÌÖúÏúºÎ°ú ÌôïÏû•ÌïòÎäî ÌîÑÎ°úÏ†ùÌä∏ÏûÖÎãàÎã§.

## üèóÔ∏è System Architecture (Docker Compose Integrated)

### 3-Phase Architecture

#### Phase 1: Core System (Profile: core)
```
ÏùåÏÑ± ÏûÖÎ†• ‚Üí STT(Whisper Medium) ‚Üí ÌÖçÏä§Ìä∏ ‚Üí vLLM(Qwen2.5-7B) ‚Üí ÌÇ§ÏõåÎìú Ï∂îÏ∂ú
```
**Services**: vllm-server, stt-streaming, redis

#### Phase 2: RAG Integration (Profile: rag)
```
ÌÇ§ÏõåÎìú ‚Üí Embedding(BGE-M3) ‚Üí Qdrant Í≤ÄÏÉâ ‚Üí Í¥ÄÎ†® Î¨∏ÏÑú ‚Üí RAG Orchestrator ‚Üí Ï¶ùÍ∞ï ÏùëÎãµ
```
**Services**: +qdrant, embedding-service, rag-orchestrator

#### Phase 3: Full Production (Profile: full)
```
ÏôÑÏ†Ñ ÌÜµÌï© ÏãúÏä§ÌÖú + API Gateway + Web UI + MongoDB + Nginx
```
**Services**: +mongodb, document-processor, api-gateway, web-ui, nginx

## üê≥ Docker Compose Í∏∞Î∞ò ÌÜµÌï© Î∞∞Ìè¨

### ÏãúÏûë Î™ÖÎ†πÏñ¥
```bash
# Core ÏãúÏä§ÌÖúÎßå ÏãúÏûë (STT + LLM + Redis)
docker-compose --profile core up -d

# RAG ÏãúÏä§ÌÖú Ìè¨Ìï® ÏãúÏûë
docker-compose --profile rag up -d

# Ï†ÑÏ≤¥ ÌîÑÎ°úÎçïÏÖò ÏãúÏä§ÌÖú ÏãúÏûë
docker-compose --profile full up -d
```

### ÏÑúÎπÑÏä§ ÏïÑÌÇ§ÌÖçÏ≤ò (11Í∞ú ÏÑúÎπÑÏä§)

```yaml
services:
  # Core Services (ÌïÑÏàò)
  - vllm-server       # Qwen2.5-7B, 13GB VRAM, Port 8000
  - stt-streaming     # Whisper Medium, 5GB VRAM, Port 5000
  - redis            # Cache, 2GB RAM, Port 6379

  # RAG Services
  - qdrant           # Vector DB, 4GB RAM, Ports 6333/6334
  - embedding-service # BGE-M3, 3GB VRAM, Port 8002
  - rag-orchestrator  # RAG Logic, CPU, Port 8003

  # Full Stack Services
  - mongodb          # Metadata, 2GB RAM, Port 27017
  - document-processor # Doc Processing, CPU, Port 8004
  - api-gateway      # Central API, CPU, Port 8080
  - web-ui           # Frontend, Port 3000
  - nginx            # Load Balancer, Ports 80/443
```

### ÎÑ§Ìä∏ÏõåÌÅ¨ Íµ¨ÏÑ±
```yaml
networks:
  frontend-net:  # Web UI ‚Üî API Gateway
  backend-net:   # Core services communication
  data-net:      # Database connections
```

## üìä GPU Î©îÎ™®Î¶¨ Ìï†Îãπ Ï†ÑÎûµ (RTX 5090 32GB)

```yaml
GPU Memory Allocation:
  vLLM (Qwen2.5-7B):     13GB (40% utilization)
  STT (Whisper Medium):   5GB
  Embedding (BGE-M3):     3GB
  ----------------------------
  Total Used:            21GB
  Available Buffer:      11GB (for operations)
```

## üîÑ Data Flow Architecture

### 1. Audio Input Flow
```mermaid
User Speech ‚Üí WebSocket ‚Üí STT Service ‚Üí Text
    ‚Üì
Text ‚Üí vLLM ‚Üí Keywords Extraction
    ‚Üì
Keywords ‚Üí Query Expansion (RAG Orchestrator)
```

### 2. RAG Processing Flow
```mermaid
Expanded Query ‚Üí Embedding Service ‚Üí Query Vector
    ‚Üì
Query Vector ‚Üí Qdrant Search ‚Üí Top-K Documents
    ‚Üì
Documents ‚Üí Reranking ‚Üí Relevant Context
    ‚Üì
Context + Query ‚Üí vLLM ‚Üí Generated Response
    ‚Üì
Response ‚Üí Redis Cache ‚Üí User
```

### 3. Document Ingestion Flow
```mermaid
Documents ‚Üí Document Processor ‚Üí Chunks
    ‚Üì
Chunks ‚Üí Embedding Service ‚Üí Vectors
    ‚Üì
Vectors ‚Üí Qdrant Storage
Metadata ‚Üí MongoDB
```

## üöÄ Implementation Roadmap (4Ï£º Í≥ÑÌöç)

### Week 1: Core System Stabilization ‚úÖ
- [x] Docker Compose Í∏∞Î≥∏ Íµ¨ÏÑ±
- [x] vLLM + STT ÌÜµÌï©
- [x] Redis Ï∫êÏã± Î†àÏù¥Ïñ¥
- [x] Í∏∞Î≥∏ health monitoring

### Week 2: RAG Foundation üîÑ
- [ ] Qdrant Î≤°ÌÑ∞ DB ÏÑ§Ï†ï
- [ ] BGE-M3 ÏûÑÎ≤†Îî© ÏÑúÎπÑÏä§ Íµ¨ÌòÑ
- [ ] Í∏∞Î≥∏ RAG Orchestrator
- [ ] Î¨∏ÏÑú ingestion ÌååÏù¥ÌîÑÎùºÏù∏

### Week 3: Intelligence Layer
- [ ] Advanced RAG with reranking
- [ ] Query expansion Î∞è ÏµúÏ†ÅÌôî
- [ ] Multi-turn conversation ÏßÄÏõê
- [ ] Context management

### Week 4: Production Ready
- [ ] Nginx Î°úÎìú Î∞∏Îü∞Ïã±
- [ ] MongoDB Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Í¥ÄÎ¶¨
- [ ] Monitoring & observability
- [ ] Auto-scaling policies

## üõ†Ô∏è Technical Decisions

### Vector Database: Qdrant
```yaml
Why Qdrant:
  - ÎåÄÍ∑úÎ™® ÌôïÏû•ÏÑ± Ïö∞Ïàò
  - Built-in filtering & payload storage
  - Multiple vectors per point (hybrid search)
  - REST & gRPC APIs
  - Production-proven
```

### Embedding Model: BGE-M3
```yaml
Why BGE-M3:
  - Îã§Íµ≠Ïñ¥ ÏßÄÏõê (ÌïúÍµ≠Ïñ¥ + ÏòÅÏñ¥)
  - 1024 dimension vectors (Í∑†ÌòïÏ†Å)
  - Dense & sparse retrieval ÏßÄÏõê
  - ~2GB model size (3GB VRAM Ìï†ÎãπÏóê Ï†ÅÌï©)
```

### Caching Strategy: Redis
```yaml
Cache Levels:
  - Query Cache: ÏûêÏ£º Î¨ªÎäî ÏßàÎ¨∏ (TTL: 1h)
  - Embedding Cache: Í≥ÑÏÇ∞Îêú ÏûÑÎ≤†Îî© (TTL: 24h)
  - Session Management: Îã§Ï§ë ÌÑ¥ ÎåÄÌôî
```

### Document Processing
```yaml
Chunking Strategy:
  - Chunk Size: 512 tokens
  - Overlap: 50 tokens
  - Metadata: title, date, source, language
  - Formats: PDF, DOCX, TXT, HTML, Markdown
```

## üìÅ Project Structure

```
stt-model/
‚îú‚îÄ‚îÄ docker-compose.yml     # 11-service orchestration
‚îú‚îÄ‚îÄ Dockerfile.vllm        # vLLM server
‚îú‚îÄ‚îÄ Dockerfile.stt         # STT streaming server
‚îú‚îÄ‚îÄ Dockerfile.embedding   # BGE-M3 embedding service
‚îú‚îÄ‚îÄ Dockerfile.rag         # RAG orchestrator
‚îú‚îÄ‚îÄ Dockerfile.processor   # Document processor
‚îú‚îÄ‚îÄ Dockerfile.gateway     # API gateway
‚îú‚îÄ‚îÄ Dockerfile.webui       # React frontend
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ nginx.conf        # Nginx configuration
‚îÇ   ‚îú‚îÄ‚îÄ qdrant.yaml       # Qdrant settings
‚îÇ   ‚îî‚îÄ‚îÄ rag_config.yaml   # RAG pipeline config
‚îú‚îÄ‚îÄ rag/
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator.py   # Main RAG logic
‚îÇ   ‚îú‚îÄ‚îÄ embedding.py      # Embedding service
‚îÇ   ‚îú‚îÄ‚îÄ retriever.py      # Search & ranking
‚îÇ   ‚îî‚îÄ‚îÄ generator.py      # Response generation
‚îú‚îÄ‚îÄ processors/
‚îÇ   ‚îú‚îÄ‚îÄ document.py       # Document chunking
‚îÇ   ‚îú‚îÄ‚îÄ metadata.py       # Metadata extraction
‚îÇ   ‚îî‚îÄ‚îÄ ingestion.py      # Batch processing
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ documents/        # Source documents
‚îÇ   ‚îú‚îÄ‚îÄ cache/           # Redis cache data
‚îÇ   ‚îî‚îÄ‚îÄ qdrant/          # Vector DB storage
‚îî‚îÄ‚îÄ scripts/
    ‚îú‚îÄ‚îÄ start_core.sh     # Start core services
    ‚îú‚îÄ‚îÄ start_rag.sh      # Start with RAG
    ‚îî‚îÄ‚îÄ start_full.sh     # Full production start
```

## üîß Service Configuration Details

### vLLM Server Configuration
```python
# Dockerfile.vllm
--model Qwen/Qwen2.5-7B-Instruct
--gpu-memory-utilization 0.5  # 50% of GPU = ~16GB
--max-model-len 224
--dtype float16
--enforce-eager
--trust-remote-code
```

### Embedding Service Configuration
```python
# embedding_service.py
class EmbeddingService:
    def __init__(self):
        self.model = FlagModel('BAAI/bge-m3',
                               use_fp16=True,
                               device='cuda')
        self.batch_size = 32
        self.max_length = 512
```

### RAG Orchestrator Configuration
```python
# rag_orchestrator.py
class RAGOrchestrator:
    def __init__(self):
        self.qdrant = QdrantClient("qdrant:6333")
        self.redis = redis.Redis("redis", 6379)
        self.vllm_url = "http://vllm-server:8000"
        self.embedding_url = "http://embedding-service:8002"

    async def process(self, query, keywords):
        # 1. Query expansion
        expanded = await self.expand_query(keywords)
        # 2. Vector search
        results = await self.vector_search(expanded)
        # 3. Reranking
        reranked = await self.rerank(results, query)
        # 4. Generate response
        response = await self.generate(query, reranked)
        return response
```

## üìä Monitoring & Observability

### Health Checks
```yaml
Services Health Endpoints:
  vLLM:      GET /v1/models ‚Üí 200 OK
  STT:       GET /api/config ‚Üí {gpu: true}
  Qdrant:    GET /collections ‚Üí list
  Redis:     PING ‚Üí PONG
  Embedding: GET /health ‚Üí ready
  RAG:       GET /health ‚Üí operational
```

### Metrics Collection
```yaml
Key Metrics:
  - GPU memory usage & utilization
  - STT processing latency
  - Embedding generation time
  - LLM inference latency
  - Vector search response time
  - Cache hit rates
  - End-to-end pipeline latency
```

### Logging Strategy
```yaml
Logging:
  - Format: Structured JSON
  - Level: ERROR (prod), INFO (dev)
  - Correlation IDs for request tracing
  - Centralized to stdout for Docker
```

## üö® Alert Thresholds
```yaml
Alerts:
  - GPU memory > 90%
  - Service response time > 5s
  - Error rate > 1%
  - Cache hit rate < 50%
  - Qdrant query latency > 500ms
```

## üîê Security Considerations

```yaml
Security Measures:
  - API key management via environment variables
  - Network isolation (3 separate networks)
  - MongoDB authentication enabled
  - Redis password protection
  - Rate limiting on API Gateway
  - CORS configuration for frontend
  - SSL/TLS via Nginx
```

## üìà Performance Targets

### System Performance
```yaml
Targets:
  - Audio ‚Üí Keywords: < 1s
  - Keywords ‚Üí RAG Response: < 2s
  - Total E2E Latency: < 3s
  - Concurrent Users: 50+
  - Documents in Vector DB: 100K+
  - QPS: 100+ queries/second
```

### Quality Metrics
```yaml
Quality:
  - STT Accuracy: > 95%
  - Keyword Precision: > 85%
  - Retrieval Relevance: > 90%
  - Response Coherence: > 4.0/5.0
  - Source Attribution: 100%
```

## üéØ Quick Start Commands

```bash
# 1. Build all images
docker-compose build

# 2. Start core services only
docker-compose --profile core up -d

# 3. Check service status
docker-compose ps
docker logs vllm-server --tail 50
docker logs stt-streaming --tail 50

# 4. Add RAG capabilities
docker-compose --profile rag up -d

# 5. Full production deployment
docker-compose --profile full up -d

# 6. Monitor GPU usage
nvidia-smi --query-gpu=name,memory.used,memory.free --format=csv

# 7. Access services
# STT Web UI: http://localhost:5000
# vLLM API: http://localhost:8000
# Qdrant UI: http://localhost:6333/dashboard
# API Gateway: http://localhost:8080
# Web UI: http://localhost:3000

# 8. Stop all services
docker-compose down

# 9. Clean up volumes
docker-compose down -v
```

## üîÑ Development Workflow

### Adding Documents to Knowledge Base
```bash
# 1. Place documents in data/documents/
cp *.pdf data/documents/

# 2. Run document processor
docker-compose run document-processor python ingest.py

# 3. Verify in Qdrant
curl http://localhost:6333/collections
```

### Testing RAG Pipeline
```python
# test_rag.py
import requests

# Test STT ‚Üí Keywords
audio_data = open("test.wav", "rb").read()
response = requests.post("http://localhost:5000/process",
                         files={"audio": audio_data})

# Test RAG response
keywords = response.json()["keywords"]
rag_response = requests.post("http://localhost:8003/query",
                             json={"keywords": keywords})
print(rag_response.json())
```

## üìù Troubleshooting

### GPU Memory Issues
```bash
# Reduce vLLM memory usage
docker-compose down vllm-server
# Edit Dockerfile.vllm: --gpu-memory-utilization 0.4
docker-compose up -d vllm-server
```

### Slow Vector Search
```bash
# Check Qdrant index
curl http://localhost:6333/collections/documents
# Optimize if needed
curl -X POST http://localhost:6333/collections/documents/index
```

### Cache Issues
```bash
# Clear Redis cache
docker exec redis-cache redis-cli FLUSHALL
```

## üéØ Final Goal

**"Ïã§ÏãúÍ∞Ñ ÏùåÏÑ±ÏùÑ ÌÜµÌï¥ Ï¶âÍ∞ÅÏ†ÅÏù∏ ÏßÄÏãù Í≤ÄÏÉâÍ≥º ÏßÄÎä•Ìòï ÏùëÎãµÏùÑ Ï†úÍ≥µÌïòÎäî Ï∞®ÏÑ∏ÎåÄ ÎåÄÌôîÌòï AI ÏãúÏä§ÌÖú"**

ÏÇ¨Ïö©ÏûêÍ∞Ä ÎßêÌïòÎäî ÏàúÍ∞Ñ, ÏãúÏä§ÌÖúÏù¥:
1. Ï†ïÌôïÌïòÍ≤å Ïù∏ÏãùÌïòÍ≥† (STT)
2. ÌïµÏã¨ÏùÑ ÌååÏïÖÌïòÍ≥† (Keyword Extraction)
3. Í¥ÄÎ†® Ï†ïÎ≥¥Î•º Ï∞æÍ≥† (Vector Search)
4. Îß•ÎùΩÏùÑ Ïù¥Ìï¥ÌïòÍ≥† (RAG)
5. ÏßÄÎä•Ï†ÅÏúºÎ°ú ÏùëÎãµÌïòÎäî (LLM Generation)

ÏôÑÏ†ÑÌïú end-to-end ÏßÄÎä•Ìòï Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ Íµ¨Ï∂ïÏù¥ ÏµúÏ¢Ö Î™©ÌëúÏûÖÎãàÎã§.