# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## üéØ Project Overview

**STT + LLM + RAG Ïã§ÏãúÍ∞Ñ ÏùåÏÑ± ÏßÄÎä•Ìòï Ï†ïÎ≥¥ ÏãúÏä§ÌÖú**

Ïã§ÏãúÍ∞Ñ ÏùåÏÑ± Ïù∏Ïãù(STT)Í≥º LLM Í∏∞Î∞ò ÌÇ§ÏõåÎìú Ï∂îÏ∂úÏùÑ ÎÑòÏñ¥, Ï∂îÏ∂úÎêú ÌÇ§ÏõåÎìúÏóê ÎåÄÌïú Í¥ÄÎ†® Ï†ïÎ≥¥Î•º ÏûêÎèôÏúºÎ°ú Í≤ÄÏÉâÌïòÍ≥† Ï†úÍ≥µÌïòÎäî RAG(Retrieval-Augmented Generation) ÏãúÏä§ÌÖúÏúºÎ°ú ÌôïÏû•ÌïòÎäî ÌîÑÎ°úÏ†ùÌä∏ÏûÖÎãàÎã§.

## üèóÔ∏è System Architecture (Docker Compose Integrated)

### 3-Phase Architecture

#### Phase 1: Core System (Profile: core)
```
ÏùåÏÑ± ÏûÖÎ†• ‚Üí STT(Whisper Medium) ‚Üí ÌÖçÏä§Ìä∏ ‚Üí vLLM(Qwen2.5-7B) ‚Üí ÌÇ§ÏõåÎìú Ï∂îÏ∂ú
```
**Services**: vllm-server, stt-streaming, redis

#### Phase 2: RAG Integration (Profile: rag)
```
ÌÇ§ÏõåÎìú ‚Üí Embedding(BGE-M3) ‚Üí Qdrant Í≤ÄÏÉâ ‚Üí Í¥ÄÎ†® Î¨∏ÏÑú ‚Üí RAG Orchestrator ‚Üí Ï¶ùÍ∞ï ÏùëÎãµ
```
**Services**: +qdrant, embedding-service, rag-orchestrator

#### Phase 3: Full Production (Profile: full)
```
ÏôÑÏ†Ñ ÌÜµÌï© ÏãúÏä§ÌÖú + API Gateway + Web UI + MongoDB + Nginx
```
**Services**: +mongodb, document-processor, api-gateway, web-ui, nginx

## üê≥ Docker Compose Í∏∞Î∞ò ÌÜµÌï© Î∞∞Ìè¨

### ÏãúÏûë Î™ÖÎ†πÏñ¥
```bash
# Core ÏãúÏä§ÌÖúÎßå ÏãúÏûë (STT + LLM + Redis)
docker-compose --profile core up -d

# RAG ÏãúÏä§ÌÖú Ìè¨Ìï® ÏãúÏûë
docker-compose --profile rag up -d

# Ï†ÑÏ≤¥ ÌîÑÎ°úÎçïÏÖò ÏãúÏä§ÌÖú ÏãúÏûë
docker-compose --profile full up -d
```

### ÏÑúÎπÑÏä§ ÏïÑÌÇ§ÌÖçÏ≤ò (11Í∞ú ÏÑúÎπÑÏä§)

```yaml
services:
  # Core Services (ÌïÑÏàò)
  - vllm-server       # Qwen2.5-7B, 13GB VRAM, Port 8000
  - stt-streaming     # Whisper Medium, 5GB VRAM, Port 5000
  - redis            # Cache, 2GB RAM, Port 6379

  # RAG Services
  - qdrant           # Vector DB, 4GB RAM, Ports 6333/6334
  - embedding-service # BGE-M3, 3GB VRAM, Port 8002
  - rag-orchestrator  # RAG Logic, CPU, Port 8003

  # Full Stack Services
  - mongodb          # Metadata, 2GB RAM, Port 27017
  - document-processor # Doc Processing, CPU, Port 8004
  - api-gateway      # Central API, CPU, Port 8080
  - web-ui           # Frontend, Port 3000
  - nginx            # Load Balancer, Ports 80/443
```

### ÎÑ§Ìä∏ÏõåÌÅ¨ Íµ¨ÏÑ±
```yaml
networks:
  frontend-net:  # Web UI ‚Üî API Gateway
  backend-net:   # Core services communication
  data-net:      # Database connections
```

## üìä GPU Î©îÎ™®Î¶¨ Ìï†Îãπ Ï†ÑÎûµ (RTX 5090 32GB)

```yaml
GPU Memory Allocation:
  vLLM (Qwen2.5-7B):     13GB (40% utilization)
  STT (Whisper Medium):   5GB
  Embedding (BGE-M3):     3GB
  ----------------------------
  Total Used:            21GB
  Available Buffer:      11GB (for operations)
```

## üîÑ Data Flow Architecture

### 1. Audio Input Flow
```mermaid
User Speech ‚Üí WebSocket ‚Üí STT Service ‚Üí Text
    ‚Üì
Text ‚Üí vLLM ‚Üí Keywords Extraction
    ‚Üì
Keywords ‚Üí Query Expansion (RAG Orchestrator)
```

### 2. RAG Processing Flow
```mermaid
Expanded Query ‚Üí Embedding Service ‚Üí Query Vector
    ‚Üì
Query Vector ‚Üí Qdrant Search ‚Üí Top-K Documents
    ‚Üì
Documents ‚Üí Reranking ‚Üí Relevant Context
    ‚Üì
Context + Query ‚Üí vLLM ‚Üí Generated Response
    ‚Üì
Response ‚Üí Redis Cache ‚Üí User
```

### 3. Document Ingestion Flow
```mermaid
Documents ‚Üí Document Processor ‚Üí Chunks
    ‚Üì
Chunks ‚Üí Embedding Service ‚Üí Vectors
    ‚Üì
Vectors ‚Üí Qdrant Storage
Metadata ‚Üí MongoDB
```

## üöÄ Implementation Roadmap (4Ï£º Í≥ÑÌöç)

### Week 1: Core System Stabilization ‚úÖ
- [x] Docker Compose Í∏∞Î≥∏ Íµ¨ÏÑ±
- [x] vLLM + STT ÌÜµÌï©
- [x] Redis Ï∫êÏã± Î†àÏù¥Ïñ¥
- [x] Í∏∞Î≥∏ health monitoring

### Week 2: RAG Foundation üîÑ
- [ ] Qdrant Î≤°ÌÑ∞ DB ÏÑ§Ï†ï
- [ ] BGE-M3 ÏûÑÎ≤†Îî© ÏÑúÎπÑÏä§ Íµ¨ÌòÑ
- [ ] Í∏∞Î≥∏ RAG Orchestrator
- [ ] Î¨∏ÏÑú ingestion ÌååÏù¥ÌîÑÎùºÏù∏

### Week 3: Intelligence Layer
- [ ] Advanced RAG with reranking
- [ ] Query expansion Î∞è ÏµúÏ†ÅÌôî
- [ ] Multi-turn conversation ÏßÄÏõê
- [ ] Context management

### Week 4: Production Ready
- [ ] Nginx Î°úÎìú Î∞∏Îü∞Ïã±
- [ ] MongoDB Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Í¥ÄÎ¶¨
- [ ] Monitoring & observability
- [ ] Auto-scaling policies

## üõ†Ô∏è Technical Decisions

### Vector Database: Qdrant
```yaml
Why Qdrant:
  - ÎåÄÍ∑úÎ™® ÌôïÏû•ÏÑ± Ïö∞Ïàò
  - Built-in filtering & payload storage
  - Multiple vectors per point (hybrid search)
  - REST & gRPC APIs
  - Production-proven
```

### Embedding Model: BGE-M3
```yaml
Why BGE-M3:
  - Îã§Íµ≠Ïñ¥ ÏßÄÏõê (ÌïúÍµ≠Ïñ¥ + ÏòÅÏñ¥)
  - 1024 dimension vectors (Í∑†ÌòïÏ†Å)
  - Dense & sparse retrieval ÏßÄÏõê
  - ~2GB model size (3GB VRAM Ìï†ÎãπÏóê Ï†ÅÌï©)
```

### Caching Strategy: Redis
```yaml
Cache Levels:
  - Query Cache: ÏûêÏ£º Î¨ªÎäî ÏßàÎ¨∏ (TTL: 1h)
  - Embedding Cache: Í≥ÑÏÇ∞Îêú ÏûÑÎ≤†Îî© (TTL: 24h)
  - Session Management: Îã§Ï§ë ÌÑ¥ ÎåÄÌôî
```

### Document Processing
```yaml
Chunking Strategy:
  - Chunk Size: 512 tokens
  - Overlap: 50 tokens
  - Metadata: title, date, source, language
  - Formats: PDF, DOCX, TXT, HTML, Markdown
```

## üìÅ Project Structure

```
stt-model/
‚îú‚îÄ‚îÄ docker-compose.yml     # 11-service orchestration
‚îú‚îÄ‚îÄ Dockerfile.vllm        # vLLM server
‚îú‚îÄ‚îÄ Dockerfile.stt         # STT streaming server
‚îú‚îÄ‚îÄ Dockerfile.embedding   # BGE-M3 embedding service
‚îú‚îÄ‚îÄ Dockerfile.rag         # RAG orchestrator
‚îú‚îÄ‚îÄ Dockerfile.processor   # Document processor
‚îú‚îÄ‚îÄ Dockerfile.gateway     # API gateway
‚îú‚îÄ‚îÄ Dockerfile.webui       # React frontend
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ nginx.conf        # Nginx configuration
‚îÇ   ‚îú‚îÄ‚îÄ qdrant.yaml       # Qdrant settings
‚îÇ   ‚îî‚îÄ‚îÄ rag_config.yaml   # RAG pipeline config
‚îú‚îÄ‚îÄ rag/
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator.py   # Main RAG logic
‚îÇ   ‚îú‚îÄ‚îÄ embedding.py      # Embedding service
‚îÇ   ‚îú‚îÄ‚îÄ retriever.py      # Search & ranking
‚îÇ   ‚îî‚îÄ‚îÄ generator.py      # Response generation
‚îú‚îÄ‚îÄ processors/
‚îÇ   ‚îú‚îÄ‚îÄ document.py       # Document chunking
‚îÇ   ‚îú‚îÄ‚îÄ metadata.py       # Metadata extraction
‚îÇ   ‚îî‚îÄ‚îÄ ingestion.py      # Batch processing
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ documents/        # Source documents
‚îÇ   ‚îú‚îÄ‚îÄ cache/           # Redis cache data
‚îÇ   ‚îî‚îÄ‚îÄ qdrant/          # Vector DB storage
‚îî‚îÄ‚îÄ scripts/
    ‚îú‚îÄ‚îÄ start_core.sh     # Start core services
    ‚îú‚îÄ‚îÄ start_rag.sh      # Start with RAG
    ‚îî‚îÄ‚îÄ start_full.sh     # Full production start
```

## üîß Service Configuration Details

### vLLM Server Configuration
```python
# Dockerfile.vllm
--model Qwen/Qwen2.5-7B-Instruct
--gpu-memory-utilization 0.5  # 50% of GPU = ~16GB
--max-model-len 224
--dtype float16
--enforce-eager
--trust-remote-code
```

### Embedding Service Configuration
```python
# embedding_service.py
class EmbeddingService:
    def __init__(self):
        self.model = FlagModel('BAAI/bge-m3',
                               use_fp16=True,
                               device='cuda')
        self.batch_size = 32
        self.max_length = 512
```

### RAG Orchestrator Configuration
```python
# rag_orchestrator.py
class RAGOrchestrator:
    def __init__(self):
        self.qdrant = QdrantClient("qdrant:6333")
        self.redis = redis.Redis("redis", 6379)
        self.vllm_url = "http://vllm-server:8000"
        self.embedding_url = "http://embedding-service:8002"

    async def process(self, query, keywords):
        # 1. Query expansion
        expanded = await self.expand_query(keywords)
        # 2. Vector search
        results = await self.vector_search(expanded)
        # 3. Reranking
        reranked = await self.rerank(results, query)
        # 4. Generate response
        response = await self.generate(query, reranked)
        return response
```

## üìä Monitoring & Observability

### Health Checks
```yaml
Services Health Endpoints:
  vLLM:      GET /v1/models ‚Üí 200 OK
  STT:       GET /api/config ‚Üí {gpu: true}
  Qdrant:    GET /collections ‚Üí list
  Redis:     PING ‚Üí PONG
  Embedding: GET /health ‚Üí ready
  RAG:       GET /health ‚Üí operational
```

### Metrics Collection
```yaml
Key Metrics:
  - GPU memory usage & utilization
  - STT processing latency
  - Embedding generation time
  - LLM inference latency
  - Vector search response time
  - Cache hit rates
  - End-to-end pipeline latency
```

### Logging Strategy
```yaml
Logging:
  - Format: Structured JSON
  - Level: ERROR (prod), INFO (dev)
  - Correlation IDs for request tracing
  - Centralized to stdout for Docker
```

## üö® Alert Thresholds
```yaml
Alerts:
  - GPU memory > 90%
  - Service response time > 5s
  - Error rate > 1%
  - Cache hit rate < 50%
  - Qdrant query latency > 500ms
```

## üîê Security Considerations

```yaml
Security Measures:
  - API key management via environment variables
  - Network isolation (3 separate networks)
  - MongoDB authentication enabled
  - Redis password protection
  - Rate limiting on API Gateway
  - CORS configuration for frontend
  - SSL/TLS via Nginx
```

## üìà Performance Targets

### System Performance
```yaml
Targets:
  - Audio ‚Üí Keywords: < 1s
  - Keywords ‚Üí RAG Response: < 2s
  - Total E2E Latency: < 3s
  - Concurrent Users: 50+
  - Documents in Vector DB: 100K+
  - QPS: 100+ queries/second
```

### Quality Metrics
```yaml
Quality:
  - STT Accuracy: > 95%
  - Keyword Precision: > 85%
  - Retrieval Relevance: > 90%
  - Response Coherence: > 4.0/5.0
  - Source Attribution: 100%
```

## üéØ Quick Start Commands

```bash
# 1. Build all images
docker-compose build

# 2. Start core services only
docker-compose --profile core up -d

# 3. Check service status
docker-compose ps
docker logs vllm-server --tail 50
docker logs stt-streaming --tail 50

# 4. Add RAG capabilities
docker-compose --profile rag up -d

# 5. Full production deployment
docker-compose --profile full up -d

# 6. Monitor GPU usage
nvidia-smi --query-gpu=name,memory.used,memory.free --format=csv

# 7. Access services
# STT Web UI: http://localhost:5000
# vLLM API: http://localhost:8000
# Qdrant UI: http://localhost:6333/dashboard
# API Gateway: http://localhost:8080
# Web UI: http://localhost:3000

# 8. Stop all services
docker-compose down

# 9. Clean up volumes
docker-compose down -v
```

## üîÑ Development Workflow

### Adding Documents to Knowledge Base
```bash
# 1. Place documents in data/documents/
cp *.pdf data/documents/

# 2. Run document processor
docker-compose run document-processor python ingest.py

# 3. Verify in Qdrant
curl http://localhost:6333/collections
```

### Testing RAG Pipeline
```python
# test_rag.py
import requests

# Test STT ‚Üí Keywords
audio_data = open("test.wav", "rb").read()
response = requests.post("http://localhost:5000/process",
                         files={"audio": audio_data})

# Test RAG response
keywords = response.json()["keywords"]
rag_response = requests.post("http://localhost:8003/query",
                             json={"keywords": keywords})
print(rag_response.json())
```

## üß™ Comprehensive Test Cases

### Unit Tests

#### 1. STT Service Tests
```python
# tests/test_stt_service.py
import pytest
import requests
import base64
import json

class TestSTTService:
    def test_health_check(self):
        """Test STT service health endpoint"""
        response = requests.get("http://localhost:5000/api/config")
        assert response.status_code == 200
        assert response.json()["gpu"] == True
        assert response.json()["model"] == "medium"

    def test_audio_upload(self):
        """Test audio file upload and transcription"""
        with open("tests/samples/korean_audio.wav", "rb") as f:
            response = requests.post(
                "http://localhost:5000/process",
                files={"audio": f}
            )
        assert response.status_code == 200
        assert "text" in response.json()
        assert len(response.json()["text"]) > 0

    def test_websocket_streaming(self):
        """Test real-time WebSocket streaming"""
        import websocket
        ws = websocket.WebSocket()
        ws.connect("ws://localhost:5000/ws")

        # Send audio chunks
        with open("tests/samples/audio_chunk.raw", "rb") as f:
            audio_data = f.read()
            ws.send(json.dumps({
                "type": "audio",
                "data": base64.b64encode(audio_data).decode()
            }))

        # Receive transcription
        result = json.loads(ws.recv())
        assert result["type"] == "transcription"
        assert "text" in result

    def test_vad_detection(self):
        """Test Voice Activity Detection"""
        # Test with silence
        silence_audio = bytes(16000)  # 1 second of silence
        response = requests.post(
            "http://localhost:5000/process",
            files={"audio": ("silence.wav", silence_audio)}
        )
        assert response.json()["text"] == ""

    def test_hallucination_filter(self):
        """Test hallucination filtering"""
        # Known hallucination patterns should be filtered
        test_texts = ["Í∞êÏÇ¨Ìï©ÎãàÎã§", "MBC Îâ¥Ïä§", "Íµ¨ÎèÖÍ≥º Ï¢ãÏïÑÏöî"]
        # Test implementation would check filtering logic
```

#### 2. vLLM Keyword Extraction Tests
```python
# tests/test_keyword_extraction.py
import pytest
import requests

class TestKeywordExtraction:
    def test_keyword_extraction_korean(self):
        """Test keyword extraction from Korean text"""
        response = requests.post(
            "http://localhost:8000/v1/completions",
            json={
                "model": "Qwen/Qwen2.5-7B-Instruct",
                "prompt": "ÌÖçÏä§Ìä∏: 'Ïù∏Í≥µÏßÄÎä• Í∏∞Ïà†Ïù¥ Îπ†Î•¥Í≤å Î∞úÏ†ÑÌïòÍ≥† ÏûàÏäµÎãàÎã§'\nÌÇ§ÏõåÎìú Ï∂îÏ∂ú:",
                "max_tokens": 200,
                "temperature": 0.1
            }
        )
        assert response.status_code == 200
        result = response.json()
        assert "Ïù∏Í≥µÏßÄÎä•" in result["choices"][0]["text"]
        assert "Í∏∞Ïà†" in result["choices"][0]["text"]

    def test_keyword_importance_scoring(self):
        """Test keyword importance scoring"""
        text = "ÏÇºÏÑ±Ï†ÑÏûêÍ∞Ä ÏÉàÎ°úÏö¥ AI Ïπ©ÏùÑ Î∞úÌëúÌñàÏäµÎãàÎã§. Ïù¥ Ïπ©ÏùÄ ÎÜíÏùÄ ÏÑ±Îä•ÏùÑ Î≥¥ÏûÖÎãàÎã§."
        response = requests.post(
            "http://localhost:8000/v1/completions",
            json={
                "model": "Qwen/Qwen2.5-7B-Instruct",
                "prompt": f"ÌÖçÏä§Ìä∏ÏóêÏÑú Ï§ëÏöîÎèÑ Ï†êÏàòÏôÄ Ìï®Íªò ÌÇ§ÏõåÎìú Ï∂îÏ∂ú:\n{text}",
                "max_tokens": 200
            }
        )
        keywords = response.json()
        # Should extract "ÏÇºÏÑ±Ï†ÑÏûê", "AI Ïπ©" with high importance

    def test_multilingual_keywords(self):
        """Test Korean-English mixed keyword extraction"""
        text = "Cloud computingÍ≥º AI Í∏∞Ïà†Ïù¥ Í≤∞Ìï©Îêú ÏÑúÎπÑÏä§"
        # Test extraction of both Korean and English keywords
```

#### 3. Embedding Service Tests
```python
# tests/test_embedding_service.py
import pytest
import requests
import numpy as np

class TestEmbeddingService:
    def test_text_embedding(self):
        """Test single text embedding generation"""
        response = requests.post(
            "http://localhost:8002/embed",
            json={"text": "Ïù∏Í≥µÏßÄÎä• Í∏∞Ïà†Ïùò Î∞úÏ†Ñ"}
        )
        assert response.status_code == 200
        embedding = response.json()["embedding"]
        assert len(embedding) == 1024  # BGE-M3 dimension
        assert isinstance(embedding[0], float)

    def test_batch_embedding(self):
        """Test batch embedding processing"""
        texts = [
            "Ï≤´ Î≤àÏß∏ Î¨∏Ïû•ÏûÖÎãàÎã§",
            "Îëê Î≤àÏß∏ Î¨∏Ïû•ÏûÖÎãàÎã§",
            "ÏÑ∏ Î≤àÏß∏ Î¨∏Ïû•ÏûÖÎãàÎã§"
        ]
        response = requests.post(
            "http://localhost:8002/embed/batch",
            json={"texts": texts}
        )
        assert response.status_code == 200
        embeddings = response.json()["embeddings"]
        assert len(embeddings) == 3
        assert len(embeddings[0]) == 1024

    def test_similarity_calculation(self):
        """Test semantic similarity between embeddings"""
        # Similar texts should have high cosine similarity
        text1 = "Ïù∏Í≥µÏßÄÎä• Í∏∞Ïà†"
        text2 = "AI Í∏∞Ïà†"
        text3 = "ÎÇ†Ïî®Í∞Ä Ï¢ãÎÑ§Ïöî"

        # Get embeddings
        emb1 = requests.post("http://localhost:8002/embed",
                             json={"text": text1}).json()["embedding"]
        emb2 = requests.post("http://localhost:8002/embed",
                             json={"text": text2}).json()["embedding"]
        emb3 = requests.post("http://localhost:8002/embed",
                             json={"text": text3}).json()["embedding"]

        # Calculate similarities
        sim_12 = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
        sim_13 = np.dot(emb1, emb3) / (np.linalg.norm(emb1) * np.linalg.norm(emb3))

        assert sim_12 > sim_13  # Similar texts should have higher similarity
```

### Integration Tests

#### 4. RAG Pipeline Tests
```python
# tests/test_rag_pipeline.py
import pytest
import requests
import time

class TestRAGPipeline:
    def test_document_ingestion(self):
        """Test document ingestion pipeline"""
        # Upload a document
        with open("tests/samples/test_doc.pdf", "rb") as f:
            response = requests.post(
                "http://localhost:8004/ingest",
                files={"document": f},
                data={"metadata": '{"source": "test", "date": "2024-01-01"}'}
            )
        assert response.status_code == 200
        doc_id = response.json()["document_id"]

        # Verify in Qdrant
        qdrant_response = requests.get(
            f"http://localhost:6333/collections/documents/points/{doc_id}"
        )
        assert qdrant_response.status_code == 200

    def test_vector_search(self):
        """Test vector similarity search"""
        query = "Ïù∏Í≥µÏßÄÎä• Í∏∞Ïà†Ïùò ÏµúÏã† ÎèôÌñ•"
        response = requests.post(
            "http://localhost:8003/search",
            json={"query": query, "top_k": 5}
        )
        assert response.status_code == 200
        results = response.json()["results"]
        assert len(results) <= 5
        assert all("score" in r for r in results)
        assert all("text" in r for r in results)

    def test_rag_response_generation(self):
        """Test complete RAG response generation"""
        keywords = ["Ïù∏Í≥µÏßÄÎä•", "Î®∏Ïã†Îü¨Îãù", "Îî•Îü¨Îãù"]
        response = requests.post(
            "http://localhost:8003/generate",
            json={
                "keywords": keywords,
                "query": "Ïù∏Í≥µÏßÄÎä• Í∏∞Ïà†Ïùò Ï∞®Ïù¥Ï†êÏùÑ ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî"
            }
        )
        assert response.status_code == 200
        result = response.json()
        assert "response" in result
        assert "sources" in result
        assert len(result["sources"]) > 0

    def test_query_expansion(self):
        """Test query expansion for better retrieval"""
        original_query = "AI"
        response = requests.post(
            "http://localhost:8003/expand_query",
            json={"query": original_query}
        )
        expanded = response.json()["expanded_queries"]
        assert "Ïù∏Í≥µÏßÄÎä•" in expanded
        assert "artificial intelligence" in expanded
        assert len(expanded) > 1
```

#### 5. End-to-End Tests
```python
# tests/test_e2e.py
import pytest
import requests
import time

class TestEndToEnd:
    def test_voice_to_knowledge_response(self):
        """Test complete flow: voice ‚Üí STT ‚Üí keywords ‚Üí RAG ‚Üí response"""
        # Step 1: Upload audio
        with open("tests/samples/question_audio.wav", "rb") as f:
            stt_response = requests.post(
                "http://localhost:5000/process",
                files={"audio": f}
            )
        assert stt_response.status_code == 200
        text = stt_response.json()["text"]
        keywords = stt_response.json()["keywords"]

        # Step 2: Get RAG response
        rag_response = requests.post(
            "http://localhost:8003/query",
            json={
                "text": text,
                "keywords": keywords
            }
        )
        assert rag_response.status_code == 200
        assert "response" in rag_response.json()
        assert len(rag_response.json()["response"]) > 0

        # Measure total latency
        start = time.time()
        # ... perform operations ...
        latency = time.time() - start
        assert latency < 3.0  # Should be under 3 seconds

    def test_multi_turn_conversation(self):
        """Test multi-turn conversation with context"""
        session_id = "test_session_123"

        # First turn
        response1 = requests.post(
            "http://localhost:8003/chat",
            json={
                "session_id": session_id,
                "message": "Ïù∏Í≥µÏßÄÎä•Ïù¥ Î≠êÏïº?",
                "keywords": ["Ïù∏Í≥µÏßÄÎä•"]
            }
        )
        assert "Ïù∏Í≥µÏßÄÎä•" in response1.json()["response"]

        # Second turn (should maintain context)
        response2 = requests.post(
            "http://localhost:8003/chat",
            json={
                "session_id": session_id,
                "message": "Îçî ÏûêÏÑ∏Ìûà ÏÑ§Î™ÖÌï¥Ï§ò",
                "keywords": ["ÏûêÏÑ∏Ìûà", "ÏÑ§Î™Ö"]
            }
        )
        # Should elaborate on AI based on previous context
        assert len(response2.json()["response"]) > len(response1.json()["response"])
```

### Performance Tests

#### 6. Load and Stress Tests
```python
# tests/test_performance.py
import pytest
import requests
import concurrent.futures
import time

class TestPerformance:
    def test_concurrent_users(self):
        """Test system with concurrent users"""
        def make_request(user_id):
            response = requests.post(
                "http://localhost:5000/process",
                files={"audio": open("tests/samples/test.wav", "rb")}
            )
            return response.status_code == 200

        # Test with 50 concurrent users
        with concurrent.futures.ThreadPoolExecutor(max_workers=50) as executor:
            futures = [executor.submit(make_request, i) for i in range(50)]
            results = [f.result() for f in concurrent.futures.as_completed(futures)]

        assert all(results)
        success_rate = sum(results) / len(results)
        assert success_rate > 0.95  # 95% success rate

    def test_response_time_sla(self):
        """Test response time SLAs"""
        timings = {
            "stt_to_keywords": [],
            "keywords_to_rag": [],
            "e2e": []
        }

        for _ in range(10):
            # Measure STT ‚Üí Keywords
            start = time.time()
            stt_response = requests.post(
                "http://localhost:5000/process",
                files={"audio": open("tests/samples/test.wav", "rb")}
            )
            timings["stt_to_keywords"].append(time.time() - start)

            # Measure Keywords ‚Üí RAG
            start = time.time()
            rag_response = requests.post(
                "http://localhost:8003/query",
                json={"keywords": stt_response.json()["keywords"]}
            )
            timings["keywords_to_rag"].append(time.time() - start)

        # Check SLAs
        assert np.mean(timings["stt_to_keywords"]) < 1.0  # < 1 second
        assert np.mean(timings["keywords_to_rag"]) < 2.0  # < 2 seconds

    def test_memory_usage(self):
        """Test GPU memory usage stays within limits"""
        response = requests.get("http://localhost:5000/api/gpu_status")
        gpu_memory = response.json()["memory_used_gb"]
        assert gpu_memory < 21  # Should stay under 21GB (our allocation)
```

### Failure and Recovery Tests

#### 7. Error Handling Tests
```python
# tests/test_error_handling.py
import pytest
import requests

class TestErrorHandling:
    def test_invalid_audio_format(self):
        """Test handling of invalid audio formats"""
        response = requests.post(
            "http://localhost:5000/process",
            files={"audio": ("test.txt", b"not audio data")}
        )
        assert response.status_code == 400
        assert "error" in response.json()

    def test_service_unavailable_recovery(self):
        """Test graceful degradation when services are down"""
        # Simulate vLLM being down
        # System should still do STT even without keyword extraction
        pass

    def test_qdrant_connection_failure(self):
        """Test fallback when vector DB is unavailable"""
        # Should fallback to basic keyword matching
        pass

    def test_rate_limiting(self):
        """Test rate limiting protection"""
        # Make 100 rapid requests
        responses = []
        for _ in range(100):
            r = requests.post("http://localhost:8080/api/query", json={})
            responses.append(r.status_code)

        # Should get rate limited (429) after threshold
        assert 429 in responses
```

### Data Quality Tests

#### 8. Accuracy and Quality Tests
```python
# tests/test_quality.py
import pytest
import requests

class TestQuality:
    def test_stt_accuracy(self):
        """Test STT accuracy with ground truth"""
        test_cases = [
            ("tests/samples/clear_korean.wav", "ÏïàÎÖïÌïòÏÑ∏Ïöî Ïò§Îäò ÎÇ†Ïî®Í∞Ä Ï¢ãÎÑ§Ïöî"),
            ("tests/samples/technical_terms.wav", "Ïù∏Í≥µÏßÄÎä•Í≥º Î®∏Ïã†Îü¨Îãù Í∏∞Ïà†"),
        ]

        for audio_file, expected_text in test_cases:
            with open(audio_file, "rb") as f:
                response = requests.post(
                    "http://localhost:5000/process",
                    files={"audio": f}
                )
            actual_text = response.json()["text"]

            # Calculate accuracy (e.g., using edit distance)
            accuracy = calculate_accuracy(expected_text, actual_text)
            assert accuracy > 0.95  # 95% accuracy threshold

    def test_retrieval_relevance(self):
        """Test retrieval relevance scoring"""
        # Test with known documents and queries
        test_queries = [
            ("Ïù∏Í≥µÏßÄÎä•Ïùò Ï†ïÏùò", ["ai_definition.pdf", "ml_basics.pdf"]),
            ("Îî•Îü¨Îãù ÏïåÍ≥†Î¶¨Ï¶ò", ["deep_learning.pdf", "neural_networks.pdf"])
        ]

        for query, expected_docs in test_queries:
            response = requests.post(
                "http://localhost:8003/search",
                json={"query": query, "top_k": 5}
            )
            retrieved = [r["source"] for r in response.json()["results"]]

            # Check if expected documents are in top results
            relevance = len(set(expected_docs) & set(retrieved[:2])) / len(expected_docs)
            assert relevance > 0.8  # 80% relevance
```

## üß™ Testing Infrastructure

### Test Data Preparation
```bash
# scripts/prepare_test_data.sh
#!/bin/bash

# Create test directories
mkdir -p tests/samples
mkdir -p tests/expected_outputs

# Download sample audio files
wget -O tests/samples/korean_audio.wav https://example.com/sample1.wav
wget -O tests/samples/clear_korean.wav https://example.com/sample2.wav

# Generate test documents
python scripts/generate_test_docs.py

# Populate vector database with test data
docker-compose run document-processor python ingest_test_data.py
```

### Continuous Testing
```yaml
# .github/workflows/test.yml
name: RAG System Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v2

    - name: Start services
      run: docker-compose --profile core up -d

    - name: Run unit tests
      run: pytest tests/unit -v

    - name: Run integration tests
      run: pytest tests/integration -v

    - name: Run performance tests
      run: pytest tests/performance -v

    - name: Generate test report
      run: pytest --html=report.html --self-contained-html
```

### Test Coverage Requirements
```yaml
coverage_targets:
  unit_tests: 80%
  integration_tests: 70%
  e2e_tests: 60%
  overall: 75%
```

## üìù Troubleshooting

### GPU Memory Issues
```bash
# Reduce vLLM memory usage
docker-compose down vllm-server
# Edit Dockerfile.vllm: --gpu-memory-utilization 0.4
docker-compose up -d vllm-server
```

### Slow Vector Search
```bash
# Check Qdrant index
curl http://localhost:6333/collections/documents
# Optimize if needed
curl -X POST http://localhost:6333/collections/documents/index
```

### Cache Issues
```bash
# Clear Redis cache
docker exec redis-cache redis-cli FLUSHALL
```

## üéØ Final Goal

**"Ïã§ÏãúÍ∞Ñ ÏùåÏÑ±ÏùÑ ÌÜµÌï¥ Ï¶âÍ∞ÅÏ†ÅÏù∏ ÏßÄÏãù Í≤ÄÏÉâÍ≥º ÏßÄÎä•Ìòï ÏùëÎãµÏùÑ Ï†úÍ≥µÌïòÎäî Ï∞®ÏÑ∏ÎåÄ ÎåÄÌôîÌòï AI ÏãúÏä§ÌÖú"**

ÏÇ¨Ïö©ÏûêÍ∞Ä ÎßêÌïòÎäî ÏàúÍ∞Ñ, ÏãúÏä§ÌÖúÏù¥:
1. Ï†ïÌôïÌïòÍ≤å Ïù∏ÏãùÌïòÍ≥† (STT)
2. ÌïµÏã¨ÏùÑ ÌååÏïÖÌïòÍ≥† (Keyword Extraction)
3. Í¥ÄÎ†® Ï†ïÎ≥¥Î•º Ï∞æÍ≥† (Vector Search)
4. Îß•ÎùΩÏùÑ Ïù¥Ìï¥ÌïòÍ≥† (RAG)
5. ÏßÄÎä•Ï†ÅÏúºÎ°ú ÏùëÎãµÌïòÎäî (LLM Generation)

ÏôÑÏ†ÑÌïú end-to-end ÏßÄÎä•Ìòï Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ Íµ¨Ï∂ïÏù¥ ÏµúÏ¢Ö Î™©ÌëúÏûÖÎãàÎã§.