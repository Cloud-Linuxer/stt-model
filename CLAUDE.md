# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## ğŸ¯ Project Overview

**STT + LLM + RAG ì‹¤ì‹œê°„ ìŒì„± ì§€ëŠ¥í˜• ì •ë³´ ì‹œìŠ¤í…œ**

ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹(STT)ê³¼ LLM ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œì„ ë„˜ì–´, ì¶”ì¶œëœ í‚¤ì›Œë“œì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ê²€ìƒ‰í•˜ê³  ì œê³µí•˜ëŠ” RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œìœ¼ë¡œ í™•ì¥í•˜ëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

## ğŸ—ï¸ System Architecture (Docker Compose Integrated)

### 3-Phase Architecture

#### Phase 1: Core System (Profile: core)
```
ìŒì„± ì…ë ¥ â†’ STT(Whisper Medium) â†’ í…ìŠ¤íŠ¸ â†’ vLLM(Qwen2.5-7B) â†’ í‚¤ì›Œë“œ ì¶”ì¶œ
```
**Services**: vllm-server, stt-streaming, redis

#### Phase 2: RAG Integration (Profile: rag)
```
í‚¤ì›Œë“œ â†’ Embedding(BGE-M3) â†’ Qdrant ê²€ìƒ‰ â†’ ê´€ë ¨ ë¬¸ì„œ â†’ RAG Orchestrator â†’ ì¦ê°• ì‘ë‹µ
```
**Services**: +qdrant, embedding-service, rag-orchestrator

#### Phase 3: Full Production (Profile: full)
```
ì™„ì „ í†µí•© ì‹œìŠ¤í…œ + API Gateway + Web UI + MongoDB + Nginx
```
**Services**: +mongodb, document-processor, api-gateway, web-ui, nginx

## ğŸ³ Docker Compose ê¸°ë°˜ í†µí•© ë°°í¬

### ì‹œì‘ ëª…ë ¹ì–´
```bash
# Core ì‹œìŠ¤í…œë§Œ ì‹œì‘ (STT + LLM + Redis)
docker-compose --profile core up -d

# RAG ì‹œìŠ¤í…œ í¬í•¨ ì‹œì‘
docker-compose --profile rag up -d

# ì „ì²´ í”„ë¡œë•ì…˜ ì‹œìŠ¤í…œ ì‹œì‘
docker-compose --profile full up -d
```

### ì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ (11ê°œ ì„œë¹„ìŠ¤)

```yaml
services:
  # Core Services (í•„ìˆ˜)
  - vllm-server       # Qwen2.5-7B, 13GB VRAM, Port 8000
  - stt-streaming     # Whisper Medium, 5GB VRAM, Port 5000
  - redis            # Cache, 2GB RAM, Port 6379

  # RAG Services
  - qdrant           # Vector DB, 4GB RAM, Ports 6333/6334
  - embedding-service # BGE-M3, 3GB VRAM, Port 8002
  - rag-orchestrator  # RAG Logic, CPU, Port 8003

  # Full Stack Services
  - mongodb          # Metadata, 2GB RAM, Port 27017
  - document-processor # Doc Processing, CPU, Port 8004
  - api-gateway      # Central API, CPU, Port 8080
  - web-ui           # Frontend, Port 3000
  - nginx            # Load Balancer, Ports 80/443
```

### ë„¤íŠ¸ì›Œí¬ êµ¬ì„±
```yaml
networks:
  frontend-net:  # Web UI â†” API Gateway
  backend-net:   # Core services communication
  data-net:      # Database connections
```

## ğŸ“Š GPU ë©”ëª¨ë¦¬ í• ë‹¹ ì „ëµ (RTX 5090 32GB)

```yaml
GPU Memory Allocation:
  vLLM (Qwen2.5-7B):     13GB (40% utilization)
  STT (Whisper Medium):   5GB
  Embedding (BGE-M3):     3GB
  ----------------------------
  Total Used:            21GB
  Available Buffer:      11GB (for operations)
```

## ğŸ”„ Data Flow Architecture

### 1. Audio Input Flow
```mermaid
User Speech â†’ WebSocket â†’ STT Service â†’ Text
    â†“
Text â†’ vLLM â†’ Keywords Extraction
    â†“
Keywords â†’ Query Expansion (RAG Orchestrator)
```

### 2. RAG Processing Flow
```mermaid
Expanded Query â†’ Embedding Service â†’ Query Vector
    â†“
Query Vector â†’ Qdrant Search â†’ Top-K Documents
    â†“
Documents â†’ Reranking â†’ Relevant Context
    â†“
Context + Query â†’ vLLM â†’ Generated Response
    â†“
Response â†’ Redis Cache â†’ User
```

### 3. Document Ingestion Flow
```mermaid
Documents â†’ Document Processor â†’ Chunks
    â†“
Chunks â†’ Embedding Service â†’ Vectors
    â†“
Vectors â†’ Qdrant Storage
Metadata â†’ MongoDB
```

## ğŸš€ Implementation Roadmap (4ì£¼ ê³„íš)

### Week 1: Core System Stabilization âœ…
- [x] Docker Compose ê¸°ë³¸ êµ¬ì„±
- [x] vLLM + STT í†µí•©
- [x] Redis ìºì‹± ë ˆì´ì–´
- [x] ê¸°ë³¸ health monitoring

### Week 2: RAG Foundation ğŸ”„
- [ ] Qdrant ë²¡í„° DB ì„¤ì •
- [ ] BGE-M3 ì„ë² ë”© ì„œë¹„ìŠ¤ êµ¬í˜„
- [ ] ê¸°ë³¸ RAG Orchestrator
- [ ] ë¬¸ì„œ ingestion íŒŒì´í”„ë¼ì¸

### Week 3: Intelligence Layer
- [ ] Advanced RAG with reranking
- [ ] Query expansion ë° ìµœì í™”
- [ ] Multi-turn conversation ì§€ì›
- [ ] Context management

### Week 4: Production Ready
- [ ] Nginx ë¡œë“œ ë°¸ëŸ°ì‹±
- [ ] MongoDB ë©”íƒ€ë°ì´í„° ê´€ë¦¬
- [ ] Monitoring & observability
- [ ] Auto-scaling policies

## ğŸ› ï¸ Technical Decisions

### Vector Database: Qdrant
```yaml
Why Qdrant:
  - ëŒ€ê·œëª¨ í™•ì¥ì„± ìš°ìˆ˜
  - Built-in filtering & payload storage
  - Multiple vectors per point (hybrid search)
  - REST & gRPC APIs
  - Production-proven
```

### Embedding Model: BGE-M3
```yaml
Why BGE-M3:
  - ë‹¤êµ­ì–´ ì§€ì› (í•œêµ­ì–´ + ì˜ì–´)
  - 1024 dimension vectors (ê· í˜•ì )
  - Dense & sparse retrieval ì§€ì›
  - ~2GB model size (3GB VRAM í• ë‹¹ì— ì í•©)
```

### Caching Strategy: Redis
```yaml
Cache Levels:
  - Query Cache: ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (TTL: 1h)
  - Embedding Cache: ê³„ì‚°ëœ ì„ë² ë”© (TTL: 24h)
  - Session Management: ë‹¤ì¤‘ í„´ ëŒ€í™”
```

### Document Processing
```yaml
Chunking Strategy:
  - Chunk Size: 512 tokens
  - Overlap: 50 tokens
  - Metadata: title, date, source, language
  - Formats: PDF, DOCX, TXT, HTML, Markdown
```

## ğŸ“ Project Structure

```
stt-model/
â”œâ”€â”€ docker-compose.yml     # 11-service orchestration
â”œâ”€â”€ Dockerfile.vllm        # vLLM server
â”œâ”€â”€ Dockerfile.stt         # STT streaming server
â”œâ”€â”€ Dockerfile.embedding   # BGE-M3 embedding service
â”œâ”€â”€ Dockerfile.rag         # RAG orchestrator
â”œâ”€â”€ Dockerfile.processor   # Document processor
â”œâ”€â”€ Dockerfile.gateway     # API gateway
â”œâ”€â”€ Dockerfile.webui       # React frontend
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ nginx.conf        # Nginx configuration
â”‚   â”œâ”€â”€ qdrant.yaml       # Qdrant settings
â”‚   â””â”€â”€ rag_config.yaml   # RAG pipeline config
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ orchestrator.py   # Main RAG logic
â”‚   â”œâ”€â”€ embedding.py      # Embedding service
â”‚   â”œâ”€â”€ retriever.py      # Search & ranking
â”‚   â””â”€â”€ generator.py      # Response generation
â”œâ”€â”€ processors/
â”‚   â”œâ”€â”€ document.py       # Document chunking
â”‚   â”œâ”€â”€ metadata.py       # Metadata extraction
â”‚   â””â”€â”€ ingestion.py      # Batch processing
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/        # Source documents
â”‚   â”œâ”€â”€ cache/           # Redis cache data
â”‚   â””â”€â”€ qdrant/          # Vector DB storage
â””â”€â”€ scripts/
    â”œâ”€â”€ start_core.sh     # Start core services
    â”œâ”€â”€ start_rag.sh      # Start with RAG
    â””â”€â”€ start_full.sh     # Full production start
```

## ğŸ”§ Service Configuration Details

### vLLM Server Configuration
```python
# Dockerfile.vllm
--model Qwen/Qwen2.5-7B-Instruct
--gpu-memory-utilization 0.5  # 50% of GPU = ~16GB
--max-model-len 224
--dtype float16
--enforce-eager
--trust-remote-code
```

### Embedding Service Configuration
```python
# embedding_service.py
class EmbeddingService:
    def __init__(self):
        self.model = FlagModel('BAAI/bge-m3',
                               use_fp16=True,
                               device='cuda')
        self.batch_size = 32
        self.max_length = 512
```

### RAG Orchestrator Configuration
```python
# rag_orchestrator.py
class RAGOrchestrator:
    def __init__(self):
        self.qdrant = QdrantClient("qdrant:6333")
        self.redis = redis.Redis("redis", 6379)
        self.vllm_url = "http://vllm-server:8000"
        self.embedding_url = "http://embedding-service:8002"

    async def process(self, query, keywords):
        # 1. Query expansion
        expanded = await self.expand_query(keywords)
        # 2. Vector search
        results = await self.vector_search(expanded)
        # 3. Reranking
        reranked = await self.rerank(results, query)
        # 4. Generate response
        response = await self.generate(query, reranked)
        return response
```

## ğŸ“Š Monitoring & Observability

### Health Checks
```yaml
Services Health Endpoints:
  vLLM:      GET /v1/models â†’ 200 OK
  STT:       GET /api/config â†’ {gpu: true}
  Qdrant:    GET /collections â†’ list
  Redis:     PING â†’ PONG
  Embedding: GET /health â†’ ready
  RAG:       GET /health â†’ operational
```

### Metrics Collection
```yaml
Key Metrics:
  - GPU memory usage & utilization
  - STT processing latency
  - Embedding generation time
  - LLM inference latency
  - Vector search response time
  - Cache hit rates
  - End-to-end pipeline latency
```

### Logging Strategy
```yaml
Logging:
  - Format: Structured JSON
  - Level: ERROR (prod), INFO (dev)
  - Correlation IDs for request tracing
  - Centralized to stdout for Docker
```

## ğŸš¨ Alert Thresholds
```yaml
Alerts:
  - GPU memory > 90%
  - Service response time > 5s
  - Error rate > 1%
  - Cache hit rate < 50%
  - Qdrant query latency > 500ms
```

## ğŸ” Security Considerations

```yaml
Security Measures:
  - API key management via environment variables
  - Network isolation (3 separate networks)
  - MongoDB authentication enabled
  - Redis password protection
  - Rate limiting on API Gateway
  - CORS configuration for frontend
  - SSL/TLS via Nginx
```

## ğŸ“ˆ Performance Targets

### System Performance
```yaml
Targets:
  - Audio â†’ Keywords: < 1s
  - Keywords â†’ RAG Response: < 2s
  - Total E2E Latency: < 3s
  - Concurrent Users: 50+
  - Documents in Vector DB: 100K+
  - QPS: 100+ queries/second
```

### Quality Metrics
```yaml
Quality:
  - STT Accuracy: > 95%
  - Keyword Precision: > 85%
  - Retrieval Relevance: > 90%
  - Response Coherence: > 4.0/5.0
  - Source Attribution: 100%
```

## ğŸ¯ Quick Start Commands

```bash
# 1. Build all images
docker-compose build

# 2. Start core services only
docker-compose --profile core up -d

# 3. Check service status
docker-compose ps
docker logs vllm-server --tail 50
docker logs stt-streaming --tail 50

# 4. Add RAG capabilities
docker-compose --profile rag up -d

# 5. Full production deployment
docker-compose --profile full up -d

# 6. Monitor GPU usage
nvidia-smi --query-gpu=name,memory.used,memory.free --format=csv

# 7. Access services
# STT Web UI: http://localhost:5000
# vLLM API: http://localhost:8000
# Qdrant UI: http://localhost:6333/dashboard
# API Gateway: http://localhost:8080
# Web UI: http://localhost:3000

# 8. Stop all services
docker-compose down

# 9. Clean up volumes
docker-compose down -v
```

## ğŸ”„ Development Workflow

### Adding Documents to Knowledge Base
```bash
# 1. Place documents in data/documents/
cp *.pdf data/documents/

# 2. Run document processor
docker-compose run document-processor python ingest.py

# 3. Verify in Qdrant
curl http://localhost:6333/collections
```

### Testing RAG Pipeline
```python
# test_rag.py
import requests

# Test STT â†’ Keywords
audio_data = open("test.wav", "rb").read()
response = requests.post("http://localhost:5000/process",
                         files={"audio": audio_data})

# Test RAG response
keywords = response.json()["keywords"]
rag_response = requests.post("http://localhost:8003/query",
                             json={"keywords": keywords})
print(rag_response.json())
```

## ğŸ“ Troubleshooting

### GPU Memory Issues
```bash
# Reduce vLLM memory usage
docker-compose down vllm-server
# Edit Dockerfile.vllm: --gpu-memory-utilization 0.4
docker-compose up -d vllm-server
```

### Slow Vector Search
```bash
# Check Qdrant index
curl http://localhost:6333/collections/documents
# Optimize if needed
curl -X POST http://localhost:6333/collections/documents/index
```

### Cache Issues
```bash
# Clear Redis cache
docker exec redis-cache redis-cli FLUSHALL
```

## ğŸ¯ Final Goal

**"ì‹¤ì‹œê°„ ìŒì„±ì„ í†µí•´ ì¦‰ê°ì ì¸ ì§€ì‹ ê²€ìƒ‰ê³¼ ì§€ëŠ¥í˜• ì‘ë‹µì„ ì œê³µí•˜ëŠ” ì°¨ì„¸ëŒ€ ëŒ€í™”í˜• AI ì‹œìŠ¤í…œ"**

ì‚¬ìš©ìê°€ ë§í•˜ëŠ” ìˆœê°„, ì‹œìŠ¤í…œì´:
1. ì •í™•í•˜ê²Œ ì¸ì‹í•˜ê³  (STT)
2. í•µì‹¬ì„ íŒŒì•…í•˜ê³  (Keyword Extraction)
3. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ê³  (Vector Search)
4. ë§¥ë½ì„ ì´í•´í•˜ê³  (RAG)
5. ì§€ëŠ¥ì ìœ¼ë¡œ ì‘ë‹µí•˜ëŠ” (LLM Generation)

ì™„ì „í•œ end-to-end ì§€ëŠ¥í˜• ì–´ì‹œìŠ¤í„´íŠ¸ êµ¬ì¶•ì´ ìµœì¢… ëª©í‘œì…ë‹ˆë‹¤.