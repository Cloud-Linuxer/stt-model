#!/usr/bin/env python3
"""
ì‹¤ì‹œê°„ ìŒì„± ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì„œë²„
Real-time Audio Streaming Server with Faster-Whisper
"""

import asyncio
import json
import numpy as np
import soundfile as sf
import io
import time
from pathlib import Path
from threading import Thread, Lock
from queue import Queue
from faster_whisper import WhisperModel
import websockets
from websockets.server import serve

class RealtimeWhisperProcessor:
    """ì‹¤ì‹œê°„ Whisper ì²˜ë¦¬ê¸°"""

    def __init__(self, model_name="large-v3", device="cuda", language="auto"):
        """
        Args:
            model_name: Whisper ëª¨ë¸ í¬ê¸°
            device: cuda ë˜ëŠ” cpu
            language: ì–¸ì–´ ì„¤ì • (auto, ko, en, ë“±)
        """
        self.model_name = model_name
        self.device = device
        self.language = language
        self.model = None
        self.audio_buffer = []
        self.buffer_lock = Lock()
        self.processing_queue = Queue()
        self.results_queue = Queue()

        # ìŠ¤íŠ¸ë¦¬ë° ì„¤ì •
        self.sample_rate = 16000  # WhisperëŠ” 16kHz í•„ìš”
        self.chunk_duration = 5  # 5ì´ˆ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        self.min_chunk_duration = 1  # ìµœì†Œ 1ì´ˆ
        self.overlap_duration = 0.5  # 0.5ì´ˆ ì˜¤ë²„ë©

        # VAD ì„¤ì •
        self.vad_enabled = True
        self.silence_threshold = 0.01
        self.silence_duration = 1.0  # 1ì´ˆ ì¹¨ë¬µì‹œ ì²˜ë¦¬

        print(f"ğŸ¤ ì‹¤ì‹œê°„ Whisper í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”")
        print(f"  - ëª¨ë¸: {model_name}")
        print(f"  - ë””ë°”ì´ìŠ¤: {device}")
        print(f"  - ì–¸ì–´: {language}")

    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        print("ğŸ”„ Faster-Whisper ëª¨ë¸ ë¡œë”© ì¤‘...")

        compute_type = "float16" if self.device == "cuda" else "int8"

        try:
            self.model = WhisperModel(
                self.model_name,
                device=self.device,
                compute_type=compute_type,
                download_root="/app/models",
                device_index=0,
                num_workers=1
            )
            print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

            # ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘
            self.processing_thread = Thread(target=self._process_audio_chunks)
            self.processing_thread.daemon = True
            self.processing_thread.start()

        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def _process_audio_chunks(self):
        """ì˜¤ë””ì˜¤ ì²­í¬ ì²˜ë¦¬ ìŠ¤ë ˆë“œ"""
        print("ğŸ¯ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘")

        while True:
            if not self.processing_queue.empty():
                audio_chunk, timestamp = self.processing_queue.get()

                try:
                    # Whisperë¡œ ì²˜ë¦¬
                    result = self._transcribe_chunk(audio_chunk, timestamp)
                    if result:
                        self.results_queue.put(result)
                except Exception as e:
                    print(f"âŒ ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            else:
                time.sleep(0.1)

    def _transcribe_chunk(self, audio_chunk, timestamp):
        """ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""

        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ (Faster-WhisperëŠ” íŒŒì¼ í•„ìš”)
        temp_file = f"/tmp/chunk_{timestamp}.wav"
        sf.write(temp_file, audio_chunk, self.sample_rate)

        try:
            # ì–¸ì–´ ì„¤ì •
            if self.language == "auto":
                language = None
            else:
                language = self.language

            # Transcribe
            segments, info = self.model.transcribe(
                temp_file,
                language=language,
                task="transcribe",
                beam_size=3,  # ì‹¤ì‹œê°„ì„ ìœ„í•´ ì¤„ì„
                best_of=3,
                patience=0.5,
                temperature=0.0,
                vad_filter=self.vad_enabled,
                vad_parameters=dict(
                    threshold=0.5,
                    min_speech_duration_ms=250,
                    min_silence_duration_ms=500,
                    speech_pad_ms=200
                ),
                initial_prompt="í•œêµ­ì–´ì™€ ì˜ì–´ë¥¼ ì •í™•í•˜ê²Œ ì¸ì‹í•˜ì„¸ìš”." if language is None else None,
                condition_on_previous_text=False  # ì‹¤ì‹œê°„ì—ì„œëŠ” False
            )

            # ê²°ê³¼ ìˆ˜ì§‘
            text = ""
            for segment in segments:
                text += segment.text

            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            Path(temp_file).unlink(missing_ok=True)

            if text.strip():
                return {
                    "timestamp": timestamp,
                    "text": text.strip(),
                    "language": info.language,
                    "confidence": info.language_probability
                }

        except Exception as e:
            print(f"âŒ Transcribe ì˜¤ë¥˜: {e}")
            Path(temp_file).unlink(missing_ok=True)

        return None

    def add_audio_data(self, audio_data, sample_rate=None):
        """ì˜¤ë””ì˜¤ ë°ì´í„° ì¶”ê°€"""

        # ìƒ˜í”Œë ˆì´íŠ¸ ë³€í™˜
        if sample_rate and sample_rate != self.sample_rate:
            # Resample to 16kHz
            import librosa
            audio_data = librosa.resample(
                audio_data,
                orig_sr=sample_rate,
                target_sr=self.sample_rate
            )

        # ë²„í¼ì— ì¶”ê°€
        with self.buffer_lock:
            self.audio_buffer.extend(audio_data)

            # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìŒ“ì´ë©´ ì²˜ë¦¬
            buffer_duration = len(self.audio_buffer) / self.sample_rate

            if buffer_duration >= self.chunk_duration:
                # ì²­í¬ ì¶”ì¶œ
                chunk_samples = int(self.chunk_duration * self.sample_rate)
                chunk = np.array(self.audio_buffer[:chunk_samples])

                # íì— ì¶”ê°€
                timestamp = time.time()
                self.processing_queue.put((chunk, timestamp))

                # ì˜¤ë²„ë©ì„ ê³ ë ¤í•œ ë²„í¼ ì—…ë°ì´íŠ¸
                overlap_samples = int(self.overlap_duration * self.sample_rate)
                self.audio_buffer = self.audio_buffer[chunk_samples - overlap_samples:]

                print(f"ğŸ“¦ ì²­í¬ ì²˜ë¦¬ íì— ì¶”ê°€ (ë²„í¼: {len(self.audio_buffer)/self.sample_rate:.1f}ì´ˆ)")

    def get_results(self):
        """ì²˜ë¦¬ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
        results = []
        while not self.results_queue.empty():
            results.append(self.results_queue.get())
        return results

    def flush_buffer(self):
        """ë²„í¼ ê°•ì œ ì²˜ë¦¬"""
        with self.buffer_lock:
            if len(self.audio_buffer) > self.min_chunk_duration * self.sample_rate:
                chunk = np.array(self.audio_buffer)
                timestamp = time.time()
                self.processing_queue.put((chunk, timestamp))
                self.audio_buffer = []
                print("ğŸ”„ ë²„í¼ í”ŒëŸ¬ì‹œ ì™„ë£Œ")


class WebSocketServer:
    """WebSocket ìŠ¤íŠ¸ë¦¬ë° ì„œë²„"""

    def __init__(self, host="0.0.0.0", port=8765):
        self.host = host
        self.port = port
        self.processor = RealtimeWhisperProcessor()
        self.clients = set()

        print(f"\nğŸŒ WebSocket ì„œë²„ ì„¤ì •")
        print(f"  - ì£¼ì†Œ: ws://{host}:{port}")

    async def handler(self, websocket, path):
        """WebSocket ì—°ê²° ì²˜ë¦¬"""

        client_id = f"{websocket.remote_address[0]}:{websocket.remote_address[1]}"
        print(f"\nâœ… í´ë¼ì´ì–¸íŠ¸ ì—°ê²°: {client_id}")

        self.clients.add(websocket)

        try:
            async for message in websocket:
                # ë°”ì´ë„ˆë¦¬ ì˜¤ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬
                if isinstance(message, bytes):
                    # ì˜¤ë””ì˜¤ ë°ì´í„° ë””ì½”ë“œ
                    audio_array = np.frombuffer(message, dtype=np.float32)

                    # í”„ë¡œì„¸ì„œì— ì¶”ê°€
                    self.processor.add_audio_data(audio_array)

                    # ê²°ê³¼ í™•ì¸ ë° ì „ì†¡
                    results = self.processor.get_results()
                    for result in results:
                        await websocket.send(json.dumps(result, ensure_ascii=False))
                        print(f"ğŸ“ [{result['language']}] {result['text']}")

                # JSON ëª…ë ¹ ì²˜ë¦¬
                elif isinstance(message, str):
                    try:
                        data = json.loads(message)

                        if data.get("command") == "flush":
                            # ë²„í¼ í”ŒëŸ¬ì‹œ
                            self.processor.flush_buffer()
                            await websocket.send(json.dumps({"status": "flushed"}))

                        elif data.get("command") == "config":
                            # ì„¤ì • ì—…ë°ì´íŠ¸
                            if "language" in data:
                                self.processor.language = data["language"]
                            if "vad" in data:
                                self.processor.vad_enabled = data["vad"]
                            await websocket.send(json.dumps({"status": "configured"}))

                    except json.JSONDecodeError:
                        pass

        except websockets.exceptions.ConnectionClosed:
            print(f"âŒ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œ: {client_id}")

        finally:
            self.clients.remove(websocket)

    async def start(self):
        """ì„œë²„ ì‹œì‘"""

        # ëª¨ë¸ ë¡œë“œ
        self.processor.load_model()

        # WebSocket ì„œë²„ ì‹œì‘
        print(f"\nğŸš€ WebSocket ì„œë²„ ì‹œì‘: ws://{self.host}:{self.port}")
        print("ğŸ“¡ ì‹¤ì‹œê°„ ìŒì„± ìŠ¤íŠ¸ë¦¬ë° ëŒ€ê¸° ì¤‘...\n")

        async with serve(self.handler, self.host, self.port):
            await asyncio.Future()  # ë¬´í•œ ëŒ€ê¸°


def create_test_client():
    """í…ŒìŠ¤íŠ¸ìš© í´ë¼ì´ì–¸íŠ¸ ì½”ë“œ ìƒì„±"""

    client_code = '''
# í…ŒìŠ¤íŠ¸ í´ë¼ì´ì–¸íŠ¸ (Python)
import asyncio
import websockets
import numpy as np
import sounddevice as sd
import json

async def stream_microphone():
    """ë§ˆì´í¬ ìŒì„±ì„ ì„œë²„ë¡œ ìŠ¤íŠ¸ë¦¬ë°"""

    uri = "ws://localhost:8765"

    async with websockets.connect(uri) as websocket:
        print("âœ… ì„œë²„ ì—°ê²°ë¨")

        # ì„¤ì •
        await websocket.send(json.dumps({
            "command": "config",
            "language": "auto",  # ìë™ ê°ì§€
            "vad": True
        }))

        def audio_callback(indata, frames, time, status):
            """ì˜¤ë””ì˜¤ ì½œë°±"""
            if status:
                print(status)

            # Float32ë¡œ ë³€í™˜
            audio = indata[:, 0].astype(np.float32)

            # ì„œë²„ë¡œ ì „ì†¡
            asyncio.create_task(websocket.send(audio.tobytes()))

        # ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ ì‹œì‘
        with sd.InputStream(
            samplerate=16000,
            channels=1,
            callback=audio_callback,
            blocksize=1600  # 0.1ì´ˆ
        ):
            print("ğŸ¤ ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ (Ctrl+Cë¡œ ì¢…ë£Œ)")

            # ê²°ê³¼ ìˆ˜ì‹ 
            while True:
                result = await websocket.recv()
                data = json.loads(result)

                if "text" in data:
                    print(f"[{data['language']}] {data['text']}")

if __name__ == "__main__":
    asyncio.run(stream_microphone())
'''

    with open("/home/stt-model/client_example.py", "w") as f:
        f.write(client_code)

    print("ğŸ“ client_example.py ìƒì„± ì™„ë£Œ")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""

    print("=" * 60)
    print("ğŸ¤ ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì„œë²„")
    print("Faster-Whisper WebSocket Streaming Server")
    print("=" * 60)

    # í…ŒìŠ¤íŠ¸ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    create_test_client()

    # ì„œë²„ ì‹œì‘
    server = WebSocketServer()

    try:
        asyncio.run(server.start())
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ ì„œë²„ ì¢…ë£Œ")

if __name__ == "__main__":
    main()