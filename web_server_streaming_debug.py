#!/usr/bin/env python3
"""
STT + LLM í‚¤ì›Œë“œ ì¶”ì¶œ í†µí•© ì„œë²„ (ìŠ¤íŠ¸ë¦¬ë° ë²„ì „) - Debug Version
- VAD ê¸°ë°˜ ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ ê°ì§€ (ë” ë‚®ì€ ì„ê³„ê°’)
- ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
- ë” ë¯¼ê°í•œ ìŒì„± ê°ì§€
"""

import json
import time
import torch
import numpy as np
import soundfile as sf
import threading
import queue
import os
import requests
import base64
from collections import deque
from flask import Flask, render_template, request, jsonify
from flask_sock import Sock
from faster_whisper import WhisperModel
import warnings
import re
warnings.filterwarnings("ignore")

# í™˜ê²½ ë³€ìˆ˜
VLLM_API_URL = os.getenv("VLLM_API_URL", "http://localhost:8000/v1/completions")

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key'
sock = Sock(app)

# ì „ì—­ ë³€ìˆ˜
stt_processor = None
keyword_extractor = None

class VoiceActivityDetector:
    """ë” ë¯¼ê°í•œ VAD êµ¬í˜„"""

    def __init__(self, sample_rate=16000):
        self.sample_rate = sample_rate
        self.energy_threshold = 0.001  # ë§¤ìš° ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ì„¤ì •
        self.silence_duration = 0.5  # ë” ì§§ì€ ì¹¨ë¬µ ê°ì§€ ì‹œê°„ (ì´ˆ)

    def is_speech(self, audio_chunk):
        """ìŒì„±ì¸ì§€ íŒë‹¨"""
        energy = np.sqrt(np.mean(np.square(audio_chunk)))
        is_speech = energy > self.energy_threshold
        if is_speech:
            print(f"ğŸ”Š Speech detected! Energy: {energy:.6f}")
        return is_speech

class StreamingSTT:
    """ìŠ¤íŠ¸ë¦¬ë° STT ì²˜ë¦¬ with debugging"""

    def __init__(self):
        self.model = None
        self.device = None
        self.vad = VoiceActivityDetector()
        self.audio_buffer = []
        self.silence_frames = 0
        self.silence_threshold = 8  # ì•½ 0.5ì´ˆ (16000Hz / 2000ms)
        self.min_speech_length = 1600  # ìµœì†Œ 0.1ì´ˆ
        self.max_buffer_size = 160000  # ìµœëŒ€ 10ì´ˆ

        # í• ë£¨ì‹œë„¤ì´ì…˜ íŒ¨í„´
        self.hallucination_patterns = [
            "ê°ì‚¬í•©ë‹ˆë‹¤", "ì‹œì²­í•´ì£¼ì…”ì„œ", "êµ¬ë…", "ì¢‹ì•„ìš”",
            "MBC ë‰´ìŠ¤", "KBS ë‰´ìŠ¤", "ë‹¤ìŒ ì˜ìƒ", "ì˜ìƒí¸ì§‘",
            "ì´ ì‹œê° ì„¸ê³„", "ë‰´ìŠ¤", "ê¸°ìì…ë‹ˆë‹¤", "ì•µì»¤ì…ë‹ˆë‹¤"
        ]

    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        print("ğŸ”„ Whisper ëª¨ë¸ ë¡œë”© ì¤‘ (ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ - Debug)...")

        if torch.cuda.is_available():
            print(f"âœ… GPU ê°ì§€ë¨: {torch.cuda.get_device_name(0)}")
            device = "cuda"
            compute_type = "float16"
        else:
            device = "cpu"
            compute_type = "int8"

        self.model = WhisperModel(
            "medium",
            device=device,
            device_index=0 if device == "cuda" else None,
            compute_type=compute_type,
            download_root="/app/models",
            num_workers=1,
            cpu_threads=0
        )
        print(f"âœ… {device.upper()} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        self.device = device
        return True

    def process_stream(self, audio_chunk):
        """ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬"""
        if self.model is None:
            print("âš ï¸ Model not loaded!")
            return None

        # ë””ë²„ê·¸: ì˜¤ë””ì˜¤ ì²­í¬ ì •ë³´ ì¶œë ¥
        print(f"ğŸ“Š Audio chunk received: {len(audio_chunk)} samples, max: {np.max(np.abs(audio_chunk)):.6f}")

        # VAD ì²´í¬
        is_speech = self.vad.is_speech(audio_chunk)

        if is_speech:
            # ìŒì„±ì´ ê°ì§€ë˜ë©´ ë²„í¼ì— ì¶”ê°€
            self.audio_buffer.extend(audio_chunk)
            self.silence_frames = 0
            print(f"ğŸ¤ Buffer size: {len(self.audio_buffer)}")

            # ë²„í¼ í¬ê¸° ì œí•œ
            if len(self.audio_buffer) > self.max_buffer_size:
                print("ğŸ“ Processing buffer (max size reached)...")
                return self._process_buffer()

        else:
            # ì¹¨ë¬µ ì¹´ìš´íŠ¸
            self.silence_frames += 1

            # ì¶©ë¶„í•œ ì¹¨ë¬µì´ ê°ì§€ë˜ê³  ë²„í¼ì— ë°ì´í„°ê°€ ìˆìœ¼ë©´
            if self.silence_frames > self.silence_threshold and len(self.audio_buffer) > self.min_speech_length:
                print(f"ğŸ“ Processing buffer after {self.silence_frames} silence frames...")
                return self._process_buffer()

        return None

    def _process_buffer(self):
        """ë²„í¼ ì²˜ë¦¬"""
        if not self.audio_buffer:
            print("âš ï¸ Buffer is empty!")
            return None

        # ë²„í¼ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        audio_data = np.array(self.audio_buffer, dtype=np.float32)
        print(f"ğŸ”Š Processing audio: {len(audio_data)} samples")

        # ì •ê·œí™”
        max_val = np.max(np.abs(audio_data))
        if max_val > 0:
            audio_data = audio_data / max_val

        # ì„ì‹œ íŒŒì¼ ì €ì¥
        temp_file = f"/tmp/audio_{time.time()}.wav"
        sf.write(temp_file, audio_data, 16000)
        print(f"ğŸ’¾ Saved audio to: {temp_file}")

        try:
            # Whisper ì²˜ë¦¬
            print("ğŸ¯ Starting Whisper transcription...")
            segments, info = self.model.transcribe(
                temp_file,
                language="ko",
                beam_size=5,
                best_of=5,
                patience=1,
                length_penalty=1,
                temperature=[0.0],  # ë‚®ì€ temperatureë¡œ í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì†Œ
                compression_ratio_threshold=2.4,
                log_prob_threshold=-1.0,
                no_speech_threshold=0.6,
                condition_on_previous_text=False,
                word_timestamps=False,
                prepend_punctuations="\"'"Â¿([{-",
                append_punctuations="\"'.ã€‚,!?:ï¼š)]}"
            )

            # ì „ì‚¬ ê²°ê³¼ ìˆ˜ì§‘
            full_text = []
            for segment in segments:
                text = segment.text.strip()
                print(f"ğŸ”¤ Segment: {text}")
                if text and not self._is_hallucination(text):
                    full_text.append(text)

            # ë²„í¼ ì´ˆê¸°í™”
            self.audio_buffer = []

            if full_text:
                result_text = " ".join(full_text)
                print(f"âœ… Final transcription: {result_text}")
                return {
                    "text": result_text,
                    "language": info.language if info else "ko",
                    "timestamp": time.time()
                }
            else:
                print("âš ï¸ No valid text transcribed")

        except Exception as e:
            print(f"âŒ Whisper ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            self.audio_buffer = []  # ì˜¤ë¥˜ ì‹œì—ë„ ë²„í¼ ì´ˆê¸°í™”

        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_file):
                os.remove(temp_file)

        return None

    def _is_hallucination(self, text):
        """í• ë£¨ì‹œë„¤ì´ì…˜ ì²´í¬"""
        for pattern in self.hallucination_patterns:
            if pattern in text:
                print(f"âš ï¸ Hallucination detected: {text}")
                return True
        return False

class KeywordExtractor:
    """í‚¤ì›Œë“œ ì¶”ì¶œ (LLM ì‚¬ìš©)"""

    def __init__(self):
        self.min_importance = 0.3
        self.vllm_url = VLLM_API_URL

    def extract_keywords(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not text or len(text) < 5:
            print("âš ï¸ Text too short for keyword extraction")
            return []

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.

í…ìŠ¤íŠ¸: "{text}"

ì‘ë‹µ í˜•ì‹:
{{
    "keywords": [
        {{"keyword": "ë‹¨ì–´1", "importance": 0.9}},
        {{"keyword": "ë‹¨ì–´2", "importance": 0.7}}
    ]
}}

í‚¤ì›Œë“œ:"""

        try:
            print(f"ğŸ”‘ Extracting keywords from: {text[:50]}...")
            response = requests.post(
                self.vllm_url,
                json={
                    "model": "Qwen/Qwen2.5-7B-Instruct",
                    "prompt": prompt,
                    "max_tokens": 100,
                    "temperature": 0.1,
                    "stop": ["}", "}}"]
                },
                timeout=5
            )

            if response.status_code == 200:
                text_response = response.json()['choices'][0]['text']
                print(f"ğŸ“‹ LLM response: {text_response}")

                # JSON íŒŒì‹± ì‹œë„
                json_pattern = r'\{[^{}]*"keywords"[^{}]*\[[^\]]*\][^{}]*\}'
                json_match = re.search(json_pattern, text_response, re.DOTALL)

                if json_match:
                    data = json.loads(json_match.group(0))
                    keywords = data.get('keywords', [])
                    filtered = [k for k in keywords if k.get('importance', 0) >= self.min_importance]
                    print(f"âœ… Keywords extracted: {filtered}")
                    return filtered

        except Exception as e:
            print(f"âŒ LLM í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")

        return []

# WebSocket í•¸ë“¤ëŸ¬
@sock.route('/ws')
def websocket(ws):
    """WebSocket ì—°ê²° ì²˜ë¦¬"""
    print(f"ğŸ”Œ ìƒˆ WebSocket ì—°ê²°: {request.remote_addr}")

    # ì „ì—­ ë³€ìˆ˜ë¡œ ë¯¸ë¦¬ ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©
    global stt_processor, keyword_extractor

    if stt_processor is None or keyword_extractor is None:
        print("ğŸ”„ Loading models for new connection...")
        streaming_stt = StreamingSTT()
        streaming_stt.load_model()
        keyword_extractor = KeywordExtractor()
    else:
        streaming_stt = stt_processor

    audio_buffer = []
    process_chunk_size = 1600  # 100ms ë‹¨ìœ„ë¡œ ì²˜ë¦¬

    try:
        while True:
            message = ws.receive()
            if message:
                try:
                    data = json.loads(message)
                    print(f"ğŸ“¨ Received message type: {data.get('type')}")

                    if data.get('type') == 'audio':
                        # Base64 ì˜¤ë””ì˜¤ ë°ì´í„° ë””ì½”ë”©
                        audio_bytes = base64.b64decode(data['data'])

                        # ë°ì´í„° í˜•ì‹ í™•ì¸
                        format_type = data.get('format', 'float32')
                        if format_type == 'float32':
                            audio_array = np.frombuffer(audio_bytes, dtype=np.float32)
                        else:
                            audio_array = np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32) / 32768.0

                        print(f"ğŸ“Š Received audio: {len(audio_array)} samples")

                        # ë²„í¼ì— ì¶”ê°€
                        audio_buffer.extend(audio_array)

                        # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ëª¨ì´ë©´ ì²˜ë¦¬
                        while len(audio_buffer) >= process_chunk_size:
                            chunk = audio_buffer[:process_chunk_size]
                            audio_buffer = audio_buffer[process_chunk_size:]

                            # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
                            result = streaming_stt.process_stream(chunk)

                            if result:
                                print(f"ğŸ‰ Got transcription result: {result}")

                                # í‚¤ì›Œë“œ ì¶”ì¶œ
                                keywords = keyword_extractor.extract_keywords(result['text'])
                                result['keywords'] = keywords

                                # ê²°ê³¼ ì „ì†¡
                                ws.send(json.dumps({
                                    "type": "transcription",
                                    **result
                                }))
                                print(f"ğŸ“¤ Sent result to client")

                except json.JSONDecodeError as e:
                    print(f"âŒ JSON decode error: {e}")
                except Exception as e:
                    print(f"âŒ Message processing error: {e}")
                    import traceback
                    traceback.print_exc()

    except Exception as e:
        print(f"âŒ WebSocket ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print(f"ğŸ”Œ WebSocket ì—°ê²° ì¢…ë£Œ: {request.remote_addr}")

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    return render_template('index_keywords_scroll.html')

@app.route('/api/config', methods=['GET'])
def get_config():
    """ì„¤ì • ì •ë³´"""
    return jsonify({
        "sample_rate": 16000,
        "language": "ko",
        "model": "medium",
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        "gpu": torch.cuda.is_available(),
        "llm_enabled": True,
        "mode": "streaming",
        "buffer_mode": True,
        "debug": True  # Debug mode enabled
    })

@app.route('/api/test-audio', methods=['POST'])
def test_audio():
    """ì˜¤ë””ì˜¤ íŒŒì¼ í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    print("ğŸ§ª Test audio endpoint called")

    global stt_processor, keyword_extractor

    if stt_processor is None:
        stt_processor = StreamingSTT()
        stt_processor.load_model()

    if keyword_extractor is None:
        keyword_extractor = KeywordExtractor()

    # íŒŒì¼ ë°›ê¸°
    if 'audio' not in request.files:
        return jsonify({"error": "No audio file"}), 400

    audio_file = request.files['audio']

    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    temp_path = f"/tmp/test_{time.time()}.wav"
    audio_file.save(temp_path)

    try:
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        audio_data, sr = sf.read(temp_path)

        # 16kHzë¡œ ë¦¬ìƒ˜í”Œë§ (í•„ìš”í•œ ê²½ìš°)
        if sr != 16000:
            from scipy import signal
            audio_data = signal.resample(audio_data, int(len(audio_data) * 16000 / sr))

        # í”„ë¡œì„¸ì‹±
        stt_processor.audio_buffer = list(audio_data)
        result = stt_processor._process_buffer()

        if result:
            keywords = keyword_extractor.extract_keywords(result['text'])
            result['keywords'] = keywords
            return jsonify(result)
        else:
            return jsonify({"error": "No transcription"}), 500

    finally:
        if os.path.exists(temp_path):
            os.remove(temp_path)

if __name__ == '__main__':
    print("\n" + "="*60)
    print("ğŸ¤ STT ìŠ¤íŠ¸ë¦¬ë° ì„œë²„ (VAD ê¸°ë°˜ - DEBUG MODE)")
    print("Streaming STT with Voice Activity Detection - Debugging Enabled")
    print("="*60 + "\n")

    if torch.cuda.is_available():
        print(f"ğŸ–¥ï¸ GPU: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ“Š GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")

    # ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ
    print("\nğŸš€ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    stt_processor = StreamingSTT()
    stt_processor.load_model()
    keyword_extractor = KeywordExtractor()
    print("âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!\n")

    # ì„œë²„ ì‹œì‘
    print(f"ğŸŒ ì„œë²„ ì‹œì‘: http://localhost:5000")
    print(f"ğŸ”‡ VAD ê¸°ë°˜ ìŒì„± ê°ì§€ (Debug Mode - Very Sensitive)")
    print(f"ğŸ”Œ WebSocket ì—”ë“œí¬ì¸íŠ¸: ws://localhost:5000/ws")
    print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸: POST /api/test-audio")
    print("="*60 + "\n")

    app.run(host='0.0.0.0', port=5000, debug=False)