# STT + LLM ì‹¤ì‹œê°„ ìŒì„±ì¸ì‹ ë° í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œìŠ¤í…œ

ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹(STT)ê³¼ LLM ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œì„ í†µí•©í•œ ê³ ì„±ëŠ¥ AI ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥

### ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹
- **Faster-Whisper**: OpenAI Whisperë³´ë‹¤ 4ë°° ë¹ ë¥¸ ì†ë„, 50% ë©”ëª¨ë¦¬ ì ˆì•½
- **VAD(Voice Activity Detection)**: ìŒì„± êµ¬ê°„ ìë™ ê°ì§€
- **ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬**: WebSocket ê¸°ë°˜ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
- **í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€**: 37ê°œ íŒ¨í„´ í•„í„°ë§

### LLM í‚¤ì›Œë“œ ì¶”ì¶œ
- **vLLM ì„œë²„**: Qwen2.5-7B-Instruct ëª¨ë¸ë¡œ ê³ ì† ì¶”ë¡ 
- **ì‹¤ì‹œê°„ í‚¤ì›Œë“œ ì¶”ì¶œ**: ì¸ì‹ëœ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ëª…ì‚¬ ìë™ ì¶”ì¶œ
- **ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜**: ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ ìë™ ë¶„ë¥˜

### ì›¹ ì¸í„°í˜ì´ìŠ¤
- **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: ìŒì„± ì¸ì‹ ê²°ê³¼ì™€ í‚¤ì›Œë“œë¥¼ ì‹¤ì‹œê°„ í‘œì‹œ
- **ìƒíƒœ í‘œì‹œ**: GPU/LLM ìƒíƒœ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- **ìë™ ìŠ¤í¬ë¡¤**: ìƒˆë¡œìš´ ë‚´ìš© ìë™ ìŠ¤í¬ë¡¤ í‘œì‹œ

## ğŸš€ Quick Start

### 1. ì‹œìŠ¤í…œ ì‹œì‘
```bash
# ì „ì²´ ì‹œìŠ¤í…œ ì‹œì‘ (vLLM + STT)
./start_system.sh

# ê°œë³„ ì„œë¹„ìŠ¤ ì‹œì‘
./start_keywords.sh  # í‚¤ì›Œë“œ ì¶”ì¶œ ë²„ì „
```

### 2. ì›¹ ì ‘ì†
- ë¡œì»¬: http://localhost:5000
- ì™¸ë¶€ ì ‘ì†: Cloudflare Tunnel URL í™•ì¸

## ğŸ—ï¸ ì‹œìŠ¤í…œ êµ¬ì„±

### ì»´í¬ë„ŒíŠ¸
- **STT ì„œë²„**: Whisper Medium ëª¨ë¸ (GPU ê°€ì†)
- **vLLM ì„œë²„**: Qwen2.5-7B (50% GPU ë©”ëª¨ë¦¬)
- **ì›¹ ì„œë²„**: Flask + WebSocket

### GPU ë©”ëª¨ë¦¬ ì‚¬ìš© (RTX 5090 32GB ê¸°ì¤€)
- Whisper Medium: ~5GB
- vLLM (Qwen2.5-7B): ~13GB
- **ì´ ì‚¬ìš©ëŸ‰**: ~18GB

## ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°
```
.
â”œâ”€â”€ Dockerfile.vllm           # vLLM ì„œë²„ Docker ì´ë¯¸ì§€
â”œâ”€â”€ web_server_streaming.py   # ë©”ì¸ STT ìŠ¤íŠ¸ë¦¬ë° ì„œë²„
â”œâ”€â”€ web_server_keywords.py    # í‚¤ì›Œë“œ ì¶”ì¶œ ì„œë²„
â”œâ”€â”€ start_system.sh          # í†µí•© ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ templates/               # ì›¹ UI í…œí”Œë¦¿
â”‚   â”œâ”€â”€ index_keywords_scroll.html
â”‚   â””â”€â”€ index_keywords.html
â”œâ”€â”€ data/                    # í…ŒìŠ¤íŠ¸ ì˜¤ë””ì˜¤ íŒŒì¼
â”œâ”€â”€ outputs/                 # ë³€í™˜ ê²°ê³¼ ì €ì¥
â””â”€â”€ models/                  # ëª¨ë¸ ìºì‹œ
```

## ğŸ”§ ì„¤ì • ë° ìµœì í™”

### GPU ë©”ëª¨ë¦¬ ì¡°ì •
`Dockerfile.vllm`ì—ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°ì •:
```python
--gpu-memory-utilization 0.5  # GPU ë©”ëª¨ë¦¬ 50% ì‚¬ìš©
--max-model-len 224           # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
```

### Whisper ëª¨ë¸ ë³€ê²½
`web_server_streaming.py`ì—ì„œ ëª¨ë¸ í¬ê¸° ë³€ê²½:
```python
self.model = WhisperModel(
    "medium",  # tiny, base, small, medium, large-v3
    device="cuda",
    compute_type="float16"
)
```

### VAD íŒŒë¼ë¯¸í„° ì¡°ì •
```python
self.energy_threshold = 0.02  # ì—ë„ˆì§€ ì„ê³„ê°’
self.silence_duration = 1.0   # ì¹¨ë¬µ ê°ì§€ ì‹œê°„
```

## ğŸ³ Docker ëª…ë ¹ì–´

### ì´ë¯¸ì§€ ë¹Œë“œ
```bash
# vLLM ì„œë²„ ë¹Œë“œ
docker build -t vllm-server -f Dockerfile.vllm .

# STT ì„œë²„ëŠ” ê¸°ì¡´ ì´ë¯¸ì§€ ì‚¬ìš©
docker-compose build
```

### ì»¨í…Œì´ë„ˆ ê´€ë¦¬
```bash
# ìƒíƒœ í™•ì¸
docker ps

# ë¡œê·¸ í™•ì¸
docker logs vllm-server -f
docker logs stt-streaming -f

# ì¬ì‹œì‘
docker restart stt-streaming
docker restart vllm-server
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### GPU ìƒíƒœ
```bash
nvidia-smi --query-gpu=name,memory.used,memory.free --format=csv
```

### ì‹œìŠ¤í…œ ë¡œê·¸
```bash
# vLLM ë¡œê·¸
docker logs vllm-server --tail 50

# STT ë¡œê·¸
docker logs stt-streaming --tail 50
```

## ğŸ› ï¸ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
1. Whisper ëª¨ë¸ í¬ê¸° ì¤„ì´ê¸° (large â†’ medium â†’ small)
2. vLLM gpu-memory-utilization ê°’ ë‚®ì¶”ê¸°
3. max-model-len ê°’ ì¤„ì´ê¸°

### WebSocket ì—°ê²° ì‹¤íŒ¨
1. ë°©í™”ë²½ ì„¤ì • í™•ì¸
2. í¬íŠ¸ 5000ì´ ì—´ë ¤ìˆëŠ”ì§€ í™•ì¸
3. Docker ë„¤íŠ¸ì›Œí¬ ì„¤ì • í™•ì¸

### í• ë£¨ì‹œë„¤ì´ì…˜ ë°œìƒ
1. temperature ê°’ ë‚®ì¶”ê¸° (0.0 ê¶Œì¥)
2. í• ë£¨ì‹œë„¤ì´ì…˜ íŒ¨í„´ ì¶”ê°€
3. VAD íŒŒë¼ë¯¸í„° ì¡°ì •

## ğŸ“‹ API ì—”ë“œí¬ì¸íŠ¸

### REST API
- `GET /`: ì›¹ ì¸í„°í˜ì´ìŠ¤
- `GET /api/config`: ì‹œìŠ¤í…œ ì„¤ì • ì •ë³´

### WebSocket
- `/ws`: ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë°
  - ì…ë ¥: Base64 ì¸ì½”ë”©ëœ ì˜¤ë””ì˜¤ ë°ì´í„°
  - ì¶œë ¥: ì¸ì‹ëœ í…ìŠ¤íŠ¸ + í‚¤ì›Œë“œ

### vLLM API
- `POST http://localhost:8000/v1/completions`: LLM ì¶”ë¡ 
- `GET http://localhost:8000/v1/models`: ëª¨ë¸ ì •ë³´

## ğŸ”‘ ì£¼ìš” ê¸°ìˆ 

- **Faster-Whisper**: CTranslate2 ê¸°ë°˜ ìµœì í™”ëœ Whisper
- **vLLM**: PagedAttentionìœ¼ë¡œ ê³ ì† LLM ì„œë¹™
- **WebSocket**: ì‹¤ì‹œê°„ ì–‘ë°©í–¥ í†µì‹ 
- **VAD**: ì‹¤ì‹œê°„ ìŒì„± êµ¬ê°„ ê°ì§€
- **Docker**: ì»¨í…Œì´ë„ˆ ê¸°ë°˜ ë°°í¬

## ğŸ“„ ë¼ì´ì„¼ìŠ¤

MIT License

## ğŸ¤ ê¸°ì—¬

Issuesì™€ Pull Requestsë¥¼ í™˜ì˜í•©ë‹ˆë‹¤!

## ğŸ“ ë¬¸ì˜

- GitHub: [Cloud-Linuxer/stt-model](https://github.com/Cloud-Linuxer/stt-model)