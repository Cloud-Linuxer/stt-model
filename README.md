# STT + LLM ì‹¤ì‹œê°„ ìŒì„±ì¸ì‹ ë° í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œìŠ¤í…œ

ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹(STT)ê³¼ LLM ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œì„ í†µí•©í•œ ê³ ì„±ëŠ¥ AI ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥

### ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹
- **Faster-Whisper**: OpenAI Whisperë³´ë‹¤ 4ë°° ë¹ ë¥¸ ì†ë„, 50% ë©”ëª¨ë¦¬ ì ˆì•½
- **VAD(Voice Activity Detection)**: ìŒì„± êµ¬ê°„ ìë™ ê°ì§€
- **ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬**: WebSocket ê¸°ë°˜ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
- **í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€**: 37ê°œ íŒ¨í„´ í•„í„°ë§

### LLM í‚¤ì›Œë“œ ì¶”ì¶œ
- **vLLM ì„œë²„**: Qwen2.5-7B-Instruct ëª¨ë¸ë¡œ ê³ ì† ì¶”ë¡ 
- **ì‹¤ì‹œê°„ í‚¤ì›Œë“œ ì¶”ì¶œ**: ì¸ì‹ëœ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ëª…ì‚¬ ìë™ ì¶”ì¶œ
- **ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜**: ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ ìë™ ë¶„ë¥˜

### ì›¹ ì¸í„°í˜ì´ìŠ¤
- **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: ìŒì„± ì¸ì‹ ê²°ê³¼ì™€ í‚¤ì›Œë“œë¥¼ ì‹¤ì‹œê°„ í‘œì‹œ
- **ìƒíƒœ í‘œì‹œ**: GPU/LLM ìƒíƒœ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- **ìë™ ìŠ¤í¬ë¡¤**: ìƒˆë¡œìš´ ë‚´ìš© ìë™ ìŠ¤í¬ë¡¤ í‘œì‹œ

## ğŸš€ Quick Start

### 1. ì‹œìŠ¤í…œ ì‹œì‘
```bash
# ì „ì²´ ì‹œìŠ¤í…œ ì‹œì‘ (vLLM + STT)
./start_system.sh

# ê°œë³„ ì„œë¹„ìŠ¤ ì‹œì‘
./start_keywords.sh  # í‚¤ì›Œë“œ ì¶”ì¶œ ë²„ì „
```

### 2. ì›¹ ì ‘ì†
- ë¡œì»¬: http://localhost:5000
- ì™¸ë¶€ ì ‘ì†: Cloudflare Tunnel URL í™•ì¸

## ğŸ—ï¸ ì‹œìŠ¤í…œ êµ¬ì„±

### ì»´í¬ë„ŒíŠ¸
- **STT ì„œë²„**: Whisper Medium ëª¨ë¸ (GPU ê°€ì†)
- **vLLM ì„œë²„**: Qwen2.5-7B (50% GPU ë©”ëª¨ë¦¬)
- **ì›¹ ì„œë²„**: Flask + WebSocket

### GPU ë©”ëª¨ë¦¬ ì‚¬ìš© (RTX 5090 32GB ê¸°ì¤€)
- Whisper Medium: ~5GB
- vLLM (Qwen2.5-7B): ~13GB
- **ì´ ì‚¬ìš©ëŸ‰**: ~18GB

## ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°
```
.
â”œâ”€â”€ Dockerfile.vllm           # vLLM ì„œë²„ Docker ì´ë¯¸ì§€
â”œâ”€â”€ web_server_streaming.py   # ë©”ì¸ STT ìŠ¤íŠ¸ë¦¬ë° ì„œë²„
â”œâ”€â”€ web_server_keywords.py    # í‚¤ì›Œë“œ ì¶”ì¶œ ì„œë²„
â”œâ”€â”€ start_system.sh          # í†µí•© ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ templates/               # ì›¹ UI í…œí”Œë¦¿
â”‚   â”œâ”€â”€ index_keywords_scroll.html
â”‚   â””â”€â”€ index_keywords.html
â”œâ”€â”€ data/                    # í…ŒìŠ¤íŠ¸ ì˜¤ë””ì˜¤ íŒŒì¼
â”œâ”€â”€ outputs/                 # ë³€í™˜ ê²°ê³¼ ì €ì¥
â””â”€â”€ models/                  # ëª¨ë¸ ìºì‹œ
```

## ğŸ”§ ì„¤ì • ë° ìµœì í™”

### GPU ë©”ëª¨ë¦¬ ì¡°ì •
`Dockerfile.vllm`ì—ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°ì •:
```python
--gpu-memory-utilization 0.5  # GPU ë©”ëª¨ë¦¬ 50% ì‚¬ìš©
--max-model-len 224           # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
```

### Whisper ëª¨ë¸ ë³€ê²½
`web_server_streaming.py`ì—ì„œ ëª¨ë¸ í¬ê¸° ë³€ê²½:
```python
self.model = WhisperModel(
    "medium",  # tiny, base, small, medium, large-v3
    device="cuda",
    compute_type="float16"
)
```

### VAD íŒŒë¼ë¯¸í„° ì¡°ì •
```python
self.energy_threshold = 0.02  # ì—ë„ˆì§€ ì„ê³„ê°’
self.silence_duration = 1.0   # ì¹¨ë¬µ ê°ì§€ ì‹œê°„
```

## ğŸ³ Docker ëª…ë ¹ì–´

### ì´ë¯¸ì§€ ë¹Œë“œ
```bash
# vLLM ì„œë²„ ë¹Œë“œ
docker build -t vllm-server -f Dockerfile.vllm .

# STT ì„œë²„ëŠ” ê¸°ì¡´ ì´ë¯¸ì§€ ì‚¬ìš©
docker-compose build
```

### ì»¨í…Œì´ë„ˆ ê´€ë¦¬
```bash
# ìƒíƒœ í™•ì¸
docker ps

# ë¡œê·¸ í™•ì¸
docker logs vllm-server -f
docker logs stt-streaming -f

# ì¬ì‹œì‘
docker restart stt-streaming
docker restart vllm-server
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### GPU ìƒíƒœ
```bash
nvidia-smi --query-gpu=name,memory.used,memory.free --format=csv
```

### ì‹œìŠ¤í…œ ë¡œê·¸
```bash
# vLLM ë¡œê·¸
docker logs vllm-server --tail 50

# STT ë¡œê·¸
docker logs stt-streaming --tail 50
```

## ğŸ› ï¸ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ğŸ”§ ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²°

#### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
1. Whisper ëª¨ë¸ í¬ê¸° ì¤„ì´ê¸° (large â†’ medium â†’ small)
2. vLLM gpu-memory-utilization ê°’ ë‚®ì¶”ê¸°
3. max-model-len ê°’ ì¤„ì´ê¸°

#### WebSocket ì—°ê²° ì‹¤íŒ¨
1. ë°©í™”ë²½ ì„¤ì • í™•ì¸
2. í¬íŠ¸ 5000ì´ ì—´ë ¤ìˆëŠ”ì§€ í™•ì¸
3. Docker ë„¤íŠ¸ì›Œí¬ ì„¤ì • í™•ì¸

#### í• ë£¨ì‹œë„¤ì´ì…˜ ë°œìƒ
1. temperature ê°’ ë‚®ì¶”ê¸° (0.0 ê¶Œì¥)
2. í• ë£¨ì‹œë„¤ì´ì…˜ íŒ¨í„´ ì¶”ê°€
3. VAD íŒŒë¼ë¯¸í„° ì¡°ì •

### ğŸš¨ CUDNN ê²½ê³  ë° ì„œë¹„ìŠ¤ ë³µì›

#### ë¬¸ì œ ìƒí™©
- CUDNN ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ê³  ë©”ì‹œì§€ ë°œìƒ
- STT ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ë˜ëŠ” ë¶ˆì•ˆì •
- WebSocket ì—°ê²° ëŠê¹€ í˜„ìƒ

#### í•´ê²° ë°©ë²•

##### 1. ê¸°ì¡´ ì‘ë™ ì´ë¯¸ì§€ë¡œ ë³µì›
```bash
# í˜„ì¬ STT ì»¨í…Œì´ë„ˆ ì •ë¦¬
docker stop $(docker ps -q --filter "name=stt") 2>/dev/null || true
docker rm $(docker ps -aq --filter "name=stt") 2>/dev/null || true

# ë„¤íŠ¸ì›Œí¬ ìƒì„± (ì—†ëŠ” ê²½ìš°)
docker network create stt-network 2>/dev/null || true

# ì›ë˜ ì‘ë™í–ˆë˜ STT ì´ë¯¸ì§€ë¡œ ë³µì›
docker run -d \
    --name stt-streaming \
    --gpus all \
    -p 5000:5000 \
    -v /app/models:/app/models \
    -e VLLM_API_URL=http://vllm-server:8000/v1/completions \
    --network stt-network \
    stt-model-stt-streaming:latest
```

##### 2. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
```bash
# ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}" | grep -E "(vllm|stt|embedding)"

# STT ì„œë¹„ìŠ¤ ë¡œê·¸ í™•ì¸
docker logs stt-streaming --tail 20

# GPU ìƒíƒœ í™•ì¸
nvidia-smi --query-gpu=name,memory.used,memory.free,utilization.gpu --format=csv,noheader,nounits
```

##### 3. ì„œë¹„ìŠ¤ ë™ì‘ í™•ì¸
```bash
# ì„¤ì • ì •ë³´ í™•ì¸
curl -s http://localhost:5000/api/config | jq

# ì›¹ ì¸í„°í˜ì´ìŠ¤ ì ‘ì† í…ŒìŠ¤íŠ¸
curl -s -I http://localhost:5000/
```

### ğŸ³ Docker ì´ë¯¸ì§€ ê´€ë¦¬

#### ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€ í™•ì¸
```bash
# STT ê´€ë ¨ ì´ë¯¸ì§€ ëª©ë¡
docker images | grep -E "(stt|gpu)" | head -10

# ê¶Œì¥ ì´ë¯¸ì§€: stt-model-stt-streaming:latest
```

#### ë„¤íŠ¸ì›Œí¬ ì„¤ì •
```bash
# í•„ìš”í•œ ë„¤íŠ¸ì›Œí¬ê°€ ì—†ëŠ” ê²½ìš°
docker network create stt-network

# ë„¤íŠ¸ì›Œí¬ ìƒíƒœ í™•ì¸
docker network ls | grep stt
```

### âš ï¸ ì£¼ì˜ì‚¬í•­

#### ì‘ë™í•˜ëŠ” ì‹œìŠ¤í…œ ìœ ì§€
- **CUDNN ê²½ê³ ê°€ ìˆì–´ë„ ê¸°ëŠ¥ì´ ì •ìƒ ì‘ë™í•˜ë©´ êµ³ì´ ìˆ˜ì •í•˜ì§€ ë§ ê²ƒ**
- GPU ë©”ëª¨ë¦¬ì™€ STT ê¸°ëŠ¥ì´ ì •ìƒì´ë©´ ê²½ê³  ë©”ì‹œì§€ëŠ” ë¬´ì‹œ ê°€ëŠ¥
- ë¶ˆí•„ìš”í•œ ì‹œí–‰ì°©ì˜¤ë¡œ ì‘ë™í•˜ëŠ” ì‹œìŠ¤í…œì„ ë§ê°€ëœ¨ë¦¬ì§€ ë§ ê²ƒ

#### ì˜¬ë°”ë¥¸ ë¬¸ì œ ì§„ë‹¨
- WebSocket ëŠê¹€ì€ ì£¼ë¡œ í´ë¼ì´ì–¸íŠ¸ ì¸¡ ì¬ì—°ê²° ë¡œì§
- CUDNN ê²½ê³ ëŠ” ê¸°ëŠ¥ì  ë¬¸ì œê°€ ì•„ë‹ ìˆ˜ ìˆìŒ
- ë¡œê·¸ì—ì„œ ì‹¤ì œ ì˜¤ë¥˜ì™€ ë‹¨ìˆœ ê²½ê³ ë¥¼ êµ¬ë¶„í•  ê²ƒ

### ğŸ“Š ì •ìƒ ìƒíƒœ í™•ì¸ ì§€í‘œ

#### ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬
```bash
# ëª¨ë“  ì„œë¹„ìŠ¤ê°€ healthy ìƒíƒœì¸ì§€ í™•ì¸
docker ps | grep healthy

# GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì •ìƒ ë²”ìœ„ì¸ì§€ í™•ì¸ (RTX 5090 ê¸°ì¤€)
# - ì´ ì‚¬ìš©ëŸ‰: ~22GB / 32GB (68% ì‚¬ìš©ë¥ )
# - vLLM: ~13GB, STT(Whisper): ~5GB, ì„ë² ë”©: ~3GB
```

#### ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
- ì›¹ ì¸í„°í˜ì´ìŠ¤ ì ‘ì†: http://localhost:5000
- API ì‘ë‹µ: `/api/config` ì—”ë“œí¬ì¸íŠ¸ ì •ìƒ ì‘ë‹µ
- GPU ê°ì§€: ì„¤ì •ì—ì„œ `"gpu": true` í™•ì¸

## ğŸ“‹ API ì—”ë“œí¬ì¸íŠ¸

### REST API
- `GET /`: ì›¹ ì¸í„°í˜ì´ìŠ¤
- `GET /api/config`: ì‹œìŠ¤í…œ ì„¤ì • ì •ë³´

### WebSocket
- `/ws`: ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë°
  - ì…ë ¥: Base64 ì¸ì½”ë”©ëœ ì˜¤ë””ì˜¤ ë°ì´í„°
  - ì¶œë ¥: ì¸ì‹ëœ í…ìŠ¤íŠ¸ + í‚¤ì›Œë“œ

### vLLM API
- `POST http://localhost:8000/v1/completions`: LLM ì¶”ë¡ 
- `GET http://localhost:8000/v1/models`: ëª¨ë¸ ì •ë³´

## ğŸ”‘ ì£¼ìš” ê¸°ìˆ 

- **Faster-Whisper**: CTranslate2 ê¸°ë°˜ ìµœì í™”ëœ Whisper
- **vLLM**: PagedAttentionìœ¼ë¡œ ê³ ì† LLM ì„œë¹™
- **WebSocket**: ì‹¤ì‹œê°„ ì–‘ë°©í–¥ í†µì‹ 
- **VAD**: ì‹¤ì‹œê°„ ìŒì„± êµ¬ê°„ ê°ì§€
- **Docker**: ì»¨í…Œì´ë„ˆ ê¸°ë°˜ ë°°í¬

## ğŸ“„ ë¼ì´ì„¼ìŠ¤

MIT License

## ğŸ¤ ê¸°ì—¬

Issuesì™€ Pull Requestsë¥¼ í™˜ì˜í•©ë‹ˆë‹¤!

## ğŸ“ ë¬¸ì˜

- GitHub: [Cloud-Linuxer/stt-model](https://github.com/Cloud-Linuxer/stt-model)