#!/usr/bin/env python3
"""
STT + LLM í‚¤ì›Œë“œ ì¶”ì¶œ í†µí•© ì„œë²„ (ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)
- VAD ê¸°ë°˜ ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ ê°ì§€
- ì¹¨ë¬µ ê°ì§€ ì‹œ ì²˜ë¦¬
- í• ë£¨ì‹œë„¤ì´ì…˜ ìµœì†Œí™”
"""

import json
import time
import torch
import numpy as np
import soundfile as sf
import threading
import queue
import os
import requests
from collections import deque
from flask import Flask, render_template, request, jsonify
from flask_sock import Sock
from faster_whisper import WhisperModel
import warnings
warnings.filterwarnings("ignore")

# í™˜ê²½ ë³€ìˆ˜
VLLM_API_URL = os.getenv("VLLM_API_URL", "http://localhost:8000/v1/completions")

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key'
sock = Sock(app)

# ì „ì—­ ë³€ìˆ˜
stt_processor = None
keyword_extractor = None

class VoiceActivityDetector:
    """ê°„ë‹¨í•œ VAD êµ¬í˜„"""

    def __init__(self, sample_rate=16000):
        self.sample_rate = sample_rate
        self.energy_threshold = 0.02  # ì—ë„ˆì§€ ì„ê³„ê°’
        self.silence_duration = 1.0  # ì¹¨ë¬µ ê°ì§€ ì‹œê°„ (ì´ˆ)

    def is_speech(self, audio_chunk):
        """ìŒì„±ì¸ì§€ íŒë‹¨"""
        energy = np.sqrt(np.mean(np.square(audio_chunk)))
        return energy > self.energy_threshold

class StreamingSTT:
    """ìŠ¤íŠ¸ë¦¬ë° STT ì²˜ë¦¬"""

    def __init__(self):
        self.model = None
        self.device = None
        self.vad = VoiceActivityDetector()
        self.audio_buffer = []
        self.silence_frames = 0
        self.silence_threshold = 16  # ì•½ 1ì´ˆ (16000Hz / 1000ms)
        self.min_speech_length = 8000  # ìµœì†Œ 0.5ì´ˆ
        self.max_buffer_size = 160000  # ìµœëŒ€ 10ì´ˆ

        # í• ë£¨ì‹œë„¤ì´ì…˜ íŒ¨í„´
        self.hallucination_patterns = [
            "ê°ì‚¬í•©ë‹ˆë‹¤", "ì‹œì²­í•´ì£¼ì…”ì„œ", "êµ¬ë…", "ì¢‹ì•„ìš”",
            "MBC ë‰´ìŠ¤", "KBS ë‰´ìŠ¤", "ë‹¤ìŒ ì˜ìƒ", "ì˜ìƒí¸ì§‘",
            "ì´ ì‹œê° ì„¸ê³„", "ë‰´ìŠ¤", "ê¸°ìì…ë‹ˆë‹¤", "ì•µì»¤ì…ë‹ˆë‹¤"
        ]

    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        print("ğŸ”„ Whisper ëª¨ë¸ ë¡œë”© ì¤‘ (ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ)...")

        if torch.cuda.is_available():
            print(f"âœ… GPU ê°ì§€ë¨: {torch.cuda.get_device_name(0)}")
            device = "cuda"
            compute_type = "float16"
        else:
            device = "cpu"
            compute_type = "int8"

        self.model = WhisperModel(
            "medium",
            device=device,
            device_index=0 if device == "cuda" else None,
            compute_type=compute_type,
            download_root="/app/models",
            num_workers=1,
            cpu_threads=0
        )
        print(f"âœ… {device.upper()} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        self.device = device
        return True

    def process_stream(self, audio_chunk):
        """ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬"""
        if self.model is None:
            return None

        # VAD ì²´í¬
        is_speech = self.vad.is_speech(audio_chunk)

        if is_speech:
            # ìŒì„±ì´ ê°ì§€ë˜ë©´ ë²„í¼ì— ì¶”ê°€
            self.audio_buffer.extend(audio_chunk)
            self.silence_frames = 0

            # ë²„í¼ í¬ê¸° ì œí•œ
            if len(self.audio_buffer) > self.max_buffer_size:
                # ë„ˆë¬´ ê¸¸ë©´ ì²˜ë¦¬
                return self._process_buffer()

        else:
            # ì¹¨ë¬µ ì¹´ìš´íŠ¸
            self.silence_frames += 1

            # ì¶©ë¶„í•œ ì¹¨ë¬µì´ ê°ì§€ë˜ê³  ë²„í¼ì— ë°ì´í„°ê°€ ìˆìœ¼ë©´
            if self.silence_frames > self.silence_threshold and len(self.audio_buffer) > self.min_speech_length:
                return self._process_buffer()

        return None

    def _process_buffer(self):
        """ë²„í¼ ì²˜ë¦¬"""
        if not self.audio_buffer:
            return None

        # ë²„í¼ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        audio_data = np.array(self.audio_buffer, dtype=np.float32)

        # ì •ê·œí™”
        audio_data = audio_data / np.max(np.abs(audio_data) + 1e-10)

        # ì„ì‹œ íŒŒì¼ ì €ì¥
        temp_file = f"/tmp/audio_{time.time()}.wav"
        sf.write(temp_file, audio_data, 16000)

        try:
            # Whisper ì²˜ë¦¬
            segments, info = self.model.transcribe(
                temp_file,
                language="ko",
                beam_size=5,
                best_of=5,
                patience=1,
                length_penalty=1,
                temperature=[0.0],  # ë‚®ì€ temperatureë¡œ í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì†Œ
                compression_ratio_threshold=2.4,
                log_prob_threshold=-1.0,
                no_speech_threshold=0.6,
                condition_on_previous_text=False,
                initial_prompt=None,  # í”„ë¡¬í”„íŠ¸ ì—†ì´
                prefix=None,
                suppress_blank=True,
                suppress_tokens=[-1],
                without_timestamps=True,
                word_timestamps=False,
                vad_filter=True,
                vad_parameters=dict(
                    threshold=0.5,
                    min_speech_duration_ms=250,
                    max_speech_duration_s=30,
                    min_silence_duration_ms=500,
                    speech_pad_ms=100
                )
            )

            text = "".join([segment.text for segment in segments]).strip()

            # í• ë£¨ì‹œë„¤ì´ì…˜ ì²´í¬
            if text and not self._is_hallucination(text):
                print(f"ğŸ“ ì¸ì‹ëœ í…ìŠ¤íŠ¸: {text}")

                # ë²„í¼ ì´ˆê¸°í™”
                self.audio_buffer = []

                return {
                    "text": text,
                    "language": info.language,
                    "probability": info.language_probability
                }

        except Exception as e:
            print(f"âŒ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_file):
                os.remove(temp_file)

        # ë²„í¼ ì´ˆê¸°í™”
        self.audio_buffer = []
        return None

    def _is_hallucination(self, text):
        """í• ë£¨ì‹œë„¤ì´ì…˜ ì²´í¬"""
        # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸
        if len(text.strip()) < 2:
            return True

        # íŒ¨í„´ ì²´í¬
        for pattern in self.hallucination_patterns:
            if pattern in text:
                print(f"âš ï¸ í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€: {text}")
                return True

        return False

class KeywordExtractor:
    """LLM í‚¤ì›Œë“œ ì¶”ì¶œ"""

    def __init__(self):
        self.api_url = VLLM_API_URL
        self.min_importance = 0.8

    def extract_keywords(self, text):
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not text or len(text.strip()) < 10:
            return []

        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.

í…ìŠ¤íŠ¸: "{text}"

ê·œì¹™:
1. ëª…ì‚¬ë§Œ ì¶”ì¶œ (ì‚¬ëŒ, ì¥ì†Œ, íšŒì‚¬, ì œí’ˆ, ê¸°ìˆ  ì´ë¦„)
2. ìµœëŒ€ 5ê°œê¹Œì§€
3. JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ

ì‘ë‹µ í˜•ì‹:
{{"keywords": [{{"word": "ë‹¨ì–´", "category": "ì¹´í…Œê³ ë¦¬", "importance": 0.9}}]}}"""

        try:
            response = requests.post(
                self.api_url,
                json={
                    "model": "Qwen/Qwen2.5-7B-Instruct",
                    "prompt": prompt,
                    "max_tokens": 200,
                    "temperature": 0.1,
                    "stop": ["}}", "\n\n"]
                },
                timeout=5
            )

            if response.status_code == 200:
                result = response.json()
                text_response = result['choices'][0]['text']

                # JSON íŒŒì‹±
                import re
                json_pattern = r'\{[^{}]*"keywords"[^{}]*\[[^\]]*\][^{}]*\}'
                json_match = re.search(json_pattern, text_response, re.DOTALL)

                if json_match:
                    data = json.loads(json_match.group(0))
                    keywords = data.get('keywords', [])
                    filtered = [k for k in keywords if k.get('importance', 0) >= self.min_importance]
                    return filtered

        except Exception as e:
            print(f"LLM í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")

        return []

# WebSocket í•¸ë“¤ëŸ¬
@sock.route('/ws')
def websocket(ws):
    """WebSocket ì—°ê²° ì²˜ë¦¬"""
    print(f"ğŸ”Œ ìƒˆ WebSocket ì—°ê²°: {request.remote_addr}")

    # ì „ì—­ ë³€ìˆ˜ë¡œ ë¯¸ë¦¬ ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©
    global stt_processor, keyword_extractor

    if stt_processor is None or keyword_extractor is None:
        # ë§Œì•½ ì—†ìœ¼ë©´ ë¡œë“œ (fallback)
        streaming_stt = StreamingSTT()
        streaming_stt.load_model()
        keyword_extractor = KeywordExtractor()
    else:
        streaming_stt = stt_processor

    audio_buffer = []
    process_chunk_size = 1600  # 100ms ë‹¨ìœ„ë¡œ ì²˜ë¦¬

    try:
        while True:
            message = ws.receive()
            if message:
                data = json.loads(message)

                if data.get('type') == 'audio':
                    # Base64 ì˜¤ë””ì˜¤ ë°ì´í„° ë””ì½”ë”©
                    import base64
                    audio_bytes = base64.b64decode(data['data'])
                    audio_array = np.frombuffer(audio_bytes, dtype=np.float32)

                    # ë²„í¼ì— ì¶”ê°€
                    audio_buffer.extend(audio_array)

                    # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ëª¨ì´ë©´ ì²˜ë¦¬
                    while len(audio_buffer) >= process_chunk_size:
                        chunk = audio_buffer[:process_chunk_size]
                        audio_buffer = audio_buffer[process_chunk_size:]

                        # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
                        result = streaming_stt.process_stream(chunk)

                        if result:
                            # í‚¤ì›Œë“œ ì¶”ì¶œ
                            keywords = keyword_extractor.extract_keywords(result['text'])
                            result['keywords'] = keywords

                            # ê²°ê³¼ ì „ì†¡
                            ws.send(json.dumps({
                                "type": "transcription",
                                **result
                            }))

    except Exception as e:
        print(f"WebSocket ì˜¤ë¥˜: {e}")
    finally:
        print(f"ğŸ”Œ WebSocket ì—°ê²° ì¢…ë£Œ: {request.remote_addr}")

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    return render_template('index_keywords_scroll.html')

@app.route('/api/config', methods=['GET'])
def get_config():
    """ì„¤ì • ì •ë³´"""
    return jsonify({
        "sample_rate": 16000,
        "language": "ko",
        "model": "medium",  # Changed from large-v3 to medium
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        "gpu": torch.cuda.is_available(),  # Added for frontend compatibility
        "llm_enabled": True,  # LLM is enabled via vLLM server
        "mode": "streaming",
        "buffer_mode": True  # VAD buffer mode is active
    })

if __name__ == '__main__':
    print("\n" + "="*60)
    print("ğŸ¤ STT ìŠ¤íŠ¸ë¦¬ë° ì„œë²„ (VAD ê¸°ë°˜)")
    print("Streaming STT with Voice Activity Detection")
    print("="*60 + "\n")

    if torch.cuda.is_available():
        print(f"ğŸ–¥ï¸ GPU: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ“Š VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

    # Whisper ëª¨ë¸ì„ ì„œë²„ ì‹œì‘ ì‹œ ë¯¸ë¦¬ ë¡œë“œ
    print("ğŸ”„ Whisper ëª¨ë¸ ë¯¸ë¦¬ ë¡œë”© ì¤‘...")
    stt_processor = StreamingSTT()
    stt_processor.load_model()
    print("âœ… Whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    # í‚¤ì›Œë“œ ì¶”ì¶œê¸°ë„ ë¯¸ë¦¬ ì´ˆê¸°í™”
    keyword_extractor = KeywordExtractor()
    print("âœ… í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ!")

    print("\nğŸŒ ì„œë²„ ì‹œì‘: http://localhost:5000")
    print("ğŸ“¡ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ í™œì„±í™”")
    print("ğŸ”‡ VAD ê¸°ë°˜ ìŒì„± ê°ì§€\n")

    app.run(host='0.0.0.0', port=5000, debug=False)